{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipes_data_df = pd.read_csv('out/recipes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>terrina de melón con gelée de oporto</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mousse de trufa negra '87</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ensalada de pasta fresca con caviar, tempura d...</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raviolis de cigala, patatas y trufa negra</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tempura de flor de calabacín rellena de mozzar...</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year\n",
       "0               terrina de melón con gelée de oporto  1987\n",
       "1                          mousse de trufa negra '87  1987\n",
       "2  ensalada de pasta fresca con caviar, tempura d...  1987\n",
       "3          raviolis de cigala, patatas y trufa negra  1987\n",
       "4  tempura de flor de calabacín rellena de mozzar...  1987"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipes_ml_df = pd.read_csv('out/recipes_ml.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_ingredients</th>\n",
       "      <th>num_preparations</th>\n",
       "      <th>num_styles</th>\n",
       "      <th>num_techniques</th>\n",
       "      <th>num_techniquesR</th>\n",
       "      <th>num_worlds</th>\n",
       "      <th>i_chocolate troceado</th>\n",
       "      <th>i_pizza</th>\n",
       "      <th>i_jugo de trufa negra</th>\n",
       "      <th>i_fresitas liofilizadas</th>\n",
       "      <th>...</th>\n",
       "      <th>temp_CALIENTE/FRÍA</th>\n",
       "      <th>temp_HELADA/FRÍA</th>\n",
       "      <th>temp_TIBIA/CALIENTE</th>\n",
       "      <th>temp_FRÍA/HELADA</th>\n",
       "      <th>temp_CALIENTE/HELADA</th>\n",
       "      <th>temp_TIBIA</th>\n",
       "      <th>temp_HELADA/FRÍA/AMBIENTE</th>\n",
       "      <th>temp_TIBIA/AMBIENTE</th>\n",
       "      <th>w_DULCE</th>\n",
       "      <th>w_SALADO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_ingredients  num_preparations  num_styles  num_techniques  \\\n",
       "0                7                 4           1              17   \n",
       "1               15                 5           0              22   \n",
       "2               21                 5           0              29   \n",
       "3               30                10           1              56   \n",
       "4               25                 7           1              37   \n",
       "\n",
       "   num_techniquesR  num_worlds  i_chocolate troceado  i_pizza  \\\n",
       "0                0           2                     0        0   \n",
       "1                0           2                     0        0   \n",
       "2                0           4                     0        0   \n",
       "3                0           3                     0        0   \n",
       "4                0           1                     0        0   \n",
       "\n",
       "   i_jugo de trufa negra  i_fresitas liofilizadas    ...     \\\n",
       "0                      0                        0    ...      \n",
       "1                      1                        0    ...      \n",
       "2                      0                        0    ...      \n",
       "3                      1                        0    ...      \n",
       "4                      0                        0    ...      \n",
       "\n",
       "   temp_CALIENTE/FRÍA  temp_HELADA/FRÍA  temp_TIBIA/CALIENTE  \\\n",
       "0                   0                 0                    0   \n",
       "1                   0                 0                    0   \n",
       "2                   0                 0                    0   \n",
       "3                   0                 0                    0   \n",
       "4                   0                 0                    0   \n",
       "\n",
       "   temp_FRÍA/HELADA  temp_CALIENTE/HELADA  temp_TIBIA  \\\n",
       "0                 0                     0           0   \n",
       "1                 0                     0           0   \n",
       "2                 0                     0           0   \n",
       "3                 0                     0           0   \n",
       "4                 0                     0           0   \n",
       "\n",
       "   temp_HELADA/FRÍA/AMBIENTE  temp_TIBIA/AMBIENTE  w_DULCE  w_SALADO  \n",
       "0                          0                    0        0         2  \n",
       "1                          0                    0        0         2  \n",
       "2                          0                    0        0         4  \n",
       "3                          0                    0        0         3  \n",
       "4                          0                    0        0         1  \n",
       "\n",
       "[5 rows x 2737 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_ml_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = recipes_ml_df\n",
    "y = recipes_data_df.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(\n",
    "    {'train': y_train.value_counts(), 'test': y_test.value_counts(), 'total': y.value_counts()},\n",
    "    columns=['train', 'test', 'total'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>79</td>\n",
       "      <td>9</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>91</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>98</td>\n",
       "      <td>11</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>124</td>\n",
       "      <td>14</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>113</td>\n",
       "      <td>13</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>113</td>\n",
       "      <td>12</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      train  test  total\n",
       "1987     14     1     15\n",
       "1988     23     3     26\n",
       "1989     25     3     28\n",
       "1990     29     3     32\n",
       "1991     51     6     57\n",
       "1992     34     4     38\n",
       "1993     27     3     30\n",
       "1994     54     6     60\n",
       "1995     46     5     51\n",
       "1996     50     6     56\n",
       "1997     55     6     61\n",
       "1998     66     7     73\n",
       "1999     79     9     88\n",
       "2000     91    10    101\n",
       "2001     98    11    109\n",
       "2003    124    14    138\n",
       "2004    113    13    126\n",
       "2005    113    12    125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # fit only on training data\n",
    "X_train_norm = scaler.transform(X_train) # transform training data\n",
    "X_test_norm = scaler.transform(X_test) # apply same transformation to test data\n",
    "X_train_dict = {\n",
    "    'MLPClassifier': X_train_norm,\n",
    "}\n",
    "X_test_dict = {\n",
    "    'MLPClassifier': X_test_norm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_score(f, xs, ys):\n",
    "    assert(len(xs) == len(ys))\n",
    "    a = 18\n",
    "    b = sum(abs(x - y) for x, y in zip(xs, ys)) / len(xs)\n",
    "    return (f(a) - f(b)) / f(a)\n",
    "\n",
    "def my_linear_score(xs, ys):\n",
    "    return my_score(lambda x: x, xs, ys)\n",
    "\n",
    "# def my_squared_score(xs, ys):\n",
    "#     return my_score(math.sqrt, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/clf_results.pickle', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(\n",
    "    dict((clf_name, results[clf_name].best_score_) for clf_name in results),\n",
    "    index=['score']\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.858313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.938543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.941697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.942664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.938187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "KNeighborsClassifier    0.858313\n",
       "LinearSVC               0.938543\n",
       "MLPClassifier           0.941697\n",
       "RandomForestClassifier  0.942664\n",
       "SVC                     0.938187"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_index_value(xs):\n",
    "    m = max(xs)\n",
    "    indices = [i for i, j in enumerate(xs) if j == m]\n",
    "    return indices, m\n",
    "\n",
    "def min_index_value(xs):\n",
    "    m = min(xs)\n",
    "    indices = [i for i, j in enumerate(xs) if j == m]\n",
    "    return indices, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Classifier: MLPClassifier\n",
      "--------------------------------------------------------------------------------\n",
      "Best score: 0.941697191697\n",
      "Best index: 8 [0 - 23]\n",
      "--------------------------------------------------------------------------------\n",
      "Means: [ 0.91966829  0.91966829  0.89850427  0.89850427  0.82371795  0.82371795\n",
      "  0.93930606  0.93930606  0.94169719  0.94169719  0.88059626  0.88059626\n",
      "  0.92892755  0.92892755  0.91107041  0.91107041  0.84366097  0.84366097\n",
      "  0.93136956  0.93136956  0.91132479  0.91132479  0.89748677  0.89748677]\n",
      "Stds: [ 0.00879911  0.00879911  0.01294261  0.01294261  0.02565244  0.02565244\n",
      "  0.00603422  0.00603422  0.00743576  0.00743576  0.01382295  0.01382295\n",
      "  0.0069485   0.0069485   0.01145507  0.01145507  0.02196844  0.02196844\n",
      "  0.01014505  0.01014505  0.01205306  0.01205306  0.01502516  0.01502516]\n",
      "--------------------------------------------------------------------------------\n",
      "Max mean: ([8, 9], 0.94169719169719168) - Std: [(8, 0.0074357554795783402), (9, 0.0074357554795783402)]\n",
      "Min std: ([6, 7], 0.0060342236036486788) - Mean: [(6, 0.93930606430606423), (7, 0.93930606430606423)]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927966</td>\n",
       "      <td>0.927966</td>\n",
       "      <td>0.901601</td>\n",
       "      <td>0.901601</td>\n",
       "      <td>0.821563</td>\n",
       "      <td>0.821563</td>\n",
       "      <td>0.939266</td>\n",
       "      <td>0.939266</td>\n",
       "      <td>0.943974</td>\n",
       "      <td>0.943974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925141</td>\n",
       "      <td>0.925141</td>\n",
       "      <td>0.855461</td>\n",
       "      <td>0.855461</td>\n",
       "      <td>0.922787</td>\n",
       "      <td>0.922787</td>\n",
       "      <td>0.902072</td>\n",
       "      <td>0.902072</td>\n",
       "      <td>0.911959</td>\n",
       "      <td>0.911959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.914272</td>\n",
       "      <td>0.914272</td>\n",
       "      <td>0.886494</td>\n",
       "      <td>0.886494</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890805</td>\n",
       "      <td>0.890805</td>\n",
       "      <td>0.863027</td>\n",
       "      <td>0.863027</td>\n",
       "      <td>0.905651</td>\n",
       "      <td>0.905651</td>\n",
       "      <td>0.899425</td>\n",
       "      <td>0.899425</td>\n",
       "      <td>0.892720</td>\n",
       "      <td>0.892720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.920019</td>\n",
       "      <td>0.920019</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.903257</td>\n",
       "      <td>0.794061</td>\n",
       "      <td>0.794061</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.898946</td>\n",
       "      <td>0.811303</td>\n",
       "      <td>0.811303</td>\n",
       "      <td>0.933429</td>\n",
       "      <td>0.933429</td>\n",
       "      <td>0.918582</td>\n",
       "      <td>0.918582</td>\n",
       "      <td>0.896073</td>\n",
       "      <td>0.896073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.934120</td>\n",
       "      <td>0.934120</td>\n",
       "      <td>0.918879</td>\n",
       "      <td>0.918879</td>\n",
       "      <td>0.843166</td>\n",
       "      <td>0.843166</td>\n",
       "      <td>0.944936</td>\n",
       "      <td>0.944936</td>\n",
       "      <td>0.951327</td>\n",
       "      <td>0.951327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.937070</td>\n",
       "      <td>0.937070</td>\n",
       "      <td>0.914946</td>\n",
       "      <td>0.914946</td>\n",
       "      <td>0.903147</td>\n",
       "      <td>0.903147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.912844</td>\n",
       "      <td>0.912844</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.917431</td>\n",
       "      <td>0.795107</td>\n",
       "      <td>0.795107</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>0.946483</td>\n",
       "      <td>0.946483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909276</td>\n",
       "      <td>0.909276</td>\n",
       "      <td>0.832314</td>\n",
       "      <td>0.832314</td>\n",
       "      <td>0.942915</td>\n",
       "      <td>0.942915</td>\n",
       "      <td>0.927625</td>\n",
       "      <td>0.927625</td>\n",
       "      <td>0.915392</td>\n",
       "      <td>0.915392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.929387</td>\n",
       "      <td>0.929387</td>\n",
       "      <td>0.895119</td>\n",
       "      <td>0.895119</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.942887</td>\n",
       "      <td>0.942887</td>\n",
       "      <td>0.940291</td>\n",
       "      <td>0.940291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.903946</td>\n",
       "      <td>0.903946</td>\n",
       "      <td>0.874351</td>\n",
       "      <td>0.874351</td>\n",
       "      <td>0.937695</td>\n",
       "      <td>0.937695</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.892523</td>\n",
       "      <td>0.892523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.921693</td>\n",
       "      <td>0.921693</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.830159</td>\n",
       "      <td>0.830159</td>\n",
       "      <td>0.946561</td>\n",
       "      <td>0.946561</td>\n",
       "      <td>0.935979</td>\n",
       "      <td>0.935979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.826455</td>\n",
       "      <td>0.826455</td>\n",
       "      <td>0.932275</td>\n",
       "      <td>0.932275</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>0.930159</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.785791</td>\n",
       "      <td>0.785791</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>0.952457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.810363</td>\n",
       "      <td>0.810363</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.934829</td>\n",
       "      <td>0.912393</td>\n",
       "      <td>0.912393</td>\n",
       "      <td>0.868056</td>\n",
       "      <td>0.868056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.919633</td>\n",
       "      <td>0.919633</td>\n",
       "      <td>0.898598</td>\n",
       "      <td>0.898598</td>\n",
       "      <td>0.810140</td>\n",
       "      <td>0.810140</td>\n",
       "      <td>0.942287</td>\n",
       "      <td>0.942287</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923948</td>\n",
       "      <td>0.923948</td>\n",
       "      <td>0.841963</td>\n",
       "      <td>0.841963</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.873786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>0.843234</td>\n",
       "      <td>0.843234</td>\n",
       "      <td>0.943894</td>\n",
       "      <td>0.943894</td>\n",
       "      <td>0.941694</td>\n",
       "      <td>0.941694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921342</td>\n",
       "      <td>0.921342</td>\n",
       "      <td>0.871837</td>\n",
       "      <td>0.871837</td>\n",
       "      <td>0.935644</td>\n",
       "      <td>0.935644</td>\n",
       "      <td>0.918042</td>\n",
       "      <td>0.918042</td>\n",
       "      <td>0.907591</td>\n",
       "      <td>0.907591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.927966  0.927966  0.901601  0.901601  0.821563  0.821563  0.939266   \n",
       "1  0.914272  0.914272  0.886494  0.886494  0.844828  0.844828  0.930556   \n",
       "2  0.920019  0.920019  0.903257  0.903257  0.794061  0.794061  0.930556   \n",
       "3  0.934120  0.934120  0.918879  0.918879  0.843166  0.843166  0.944936   \n",
       "4  0.912844  0.912844  0.917431  0.917431  0.795107  0.795107  0.931193   \n",
       "5  0.929387  0.929387  0.895119  0.895119  0.869159  0.869159  0.942887   \n",
       "6  0.921693  0.921693  0.885714  0.885714  0.830159  0.830159  0.946561   \n",
       "7  0.903846  0.903846  0.875000  0.875000  0.785791  0.785791  0.942842   \n",
       "8  0.919633  0.919633  0.898598  0.898598  0.810140  0.810140  0.942287   \n",
       "9  0.910891  0.910891  0.900990  0.900990  0.843234  0.843234  0.943894   \n",
       "\n",
       "         7         8         9     ...           14        15        16  \\\n",
       "0  0.939266  0.943974  0.943974    ...     0.925141  0.925141  0.855461   \n",
       "1  0.930556  0.929119  0.929119    ...     0.890805  0.890805  0.863027   \n",
       "2  0.930556  0.944444  0.944444    ...     0.898946  0.898946  0.811303   \n",
       "3  0.944936  0.951327  0.951327    ...     0.911504  0.911504  0.849558   \n",
       "4  0.931193  0.946483  0.946483    ...     0.909276  0.909276  0.832314   \n",
       "5  0.942887  0.940291  0.940291    ...     0.903946  0.903946  0.874351   \n",
       "6  0.946561  0.935979  0.935979    ...     0.923810  0.923810  0.826455   \n",
       "7  0.942842  0.952457  0.952457    ...     0.904380  0.904380  0.810363   \n",
       "8  0.942287  0.930960  0.930960    ...     0.923948  0.923948  0.841963   \n",
       "9  0.943894  0.941694  0.941694    ...     0.921342  0.921342  0.871837   \n",
       "\n",
       "         17        18        19        20        21        22        23  \n",
       "0  0.855461  0.922787  0.922787  0.902072  0.902072  0.911959  0.911959  \n",
       "1  0.863027  0.905651  0.905651  0.899425  0.899425  0.892720  0.892720  \n",
       "2  0.811303  0.933429  0.933429  0.918582  0.918582  0.896073  0.896073  \n",
       "3  0.849558  0.937070  0.937070  0.914946  0.914946  0.903147  0.903147  \n",
       "4  0.832314  0.942915  0.942915  0.927625  0.927625  0.915392  0.915392  \n",
       "5  0.874351  0.937695  0.937695  0.897715  0.897715  0.892523  0.892523  \n",
       "6  0.826455  0.932275  0.932275  0.930159  0.930159  0.911111  0.911111  \n",
       "7  0.810363  0.934829  0.934829  0.912393  0.912393  0.868056  0.868056  \n",
       "8  0.841963  0.934196  0.934196  0.893204  0.893204  0.873786  0.873786  \n",
       "9  0.871837  0.935644  0.935644  0.918042  0.918042  0.907591  0.907591  \n",
       "\n",
       "[10 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Classifier: RandomForestClassifier\n",
      "--------------------------------------------------------------------------------\n",
      "Best score: 0.942663817664\n",
      "Best index: 98 [0 - 107]\n",
      "--------------------------------------------------------------------------------\n",
      "Means: [ 0.78947904  0.78947904  0.76541514  0.76541514  0.75579976  0.75579976\n",
      "  0.83786121  0.83786121  0.85597273  0.85597273  0.85933048  0.85933048\n",
      "  0.89255189  0.89255189  0.89718152  0.89718152  0.89707977  0.89707977\n",
      "  0.85129223  0.85129223  0.85907611  0.85907611  0.85927961  0.85927961\n",
      "  0.89397639  0.89397639  0.90532153  0.90532153  0.90577941  0.90577941\n",
      "  0.91381766  0.91381766  0.92094017  0.92094017  0.9231278   0.9231278\n",
      "  0.88298738  0.88298738  0.93264143  0.93264143  0.93894994  0.93894994\n",
      "  0.91442816  0.91442816  0.93579569  0.93579569  0.93935694  0.93935694\n",
      "  0.91778592  0.91778592  0.92531543  0.92531543  0.92791005  0.92791005\n",
      "  0.83867521  0.83867521  0.85871998  0.85871998  0.85332723  0.85332723\n",
      "  0.88771876  0.88771876  0.90135328  0.90135328  0.89631665  0.89631665\n",
      "  0.91397029  0.91397029  0.91625967  0.91625967  0.91575092  0.91575092\n",
      "  0.8707265   0.8707265   0.90120065  0.90120065  0.91168091  0.91168091\n",
      "  0.90867928  0.90867928  0.93259056  0.93259056  0.93569394  0.93569394\n",
      "  0.92567155  0.92567155  0.92775743  0.92775743  0.93147131  0.93147131\n",
      "  0.88873626  0.88873626  0.93335368  0.93335368  0.93910256  0.93910256\n",
      "  0.91244404  0.91244404  0.94266382  0.94266382  0.94195157  0.94195157\n",
      "  0.92205942  0.92205942  0.93309931  0.93309931  0.93228531  0.93228531]\n",
      "Stds: [ 0.01736258  0.01736258  0.01587302  0.01587302  0.01460712  0.01460712\n",
      "  0.02490311  0.02490311  0.01657152  0.01657152  0.0141353   0.0141353\n",
      "  0.01009416  0.01009416  0.01020541  0.01020541  0.01025343  0.01025343\n",
      "  0.01709795  0.01709795  0.01565707  0.01565707  0.01534212  0.01534212\n",
      "  0.00755119  0.00755119  0.00954457  0.00954457  0.01184762  0.01184762\n",
      "  0.01209852  0.01209852  0.00906248  0.00906248  0.00807793  0.00807793\n",
      "  0.00620466  0.00620466  0.00915119  0.00915119  0.00577356  0.00577356\n",
      "  0.00906156  0.00906156  0.00886359  0.00886359  0.00775841  0.00775841\n",
      "  0.00904618  0.00904618  0.00995134  0.00995134  0.00744814  0.00744814\n",
      "  0.01742131  0.01742131  0.01313141  0.01313141  0.01547179  0.01547179\n",
      "  0.01428561  0.01428561  0.01370107  0.01370107  0.01271476  0.01271476\n",
      "  0.00927478  0.00927478  0.01220895  0.01220895  0.00935269  0.00935269\n",
      "  0.01171495  0.01171495  0.01219203  0.01219203  0.01214033  0.01214033\n",
      "  0.01013494  0.01013494  0.00808112  0.00808112  0.0070046   0.0070046\n",
      "  0.00837793  0.00837793  0.00877903  0.00877903  0.00745601  0.00745601\n",
      "  0.0098744   0.0098744   0.01125836  0.01125836  0.00695395  0.00695395\n",
      "  0.00799364  0.00799364  0.00870143  0.00870143  0.0072984   0.0072984\n",
      "  0.00719262  0.00719262  0.00779755  0.00779755  0.00840441  0.00840441]\n",
      "--------------------------------------------------------------------------------\n",
      "Max mean: ([98, 99], 0.94266381766381768) - Std: [(98, 0.0087014259119277317), (99, 0.0087014259119277317)]\n",
      "Min std: ([40, 41], 0.005773559987791302) - Mean: [(40, 0.93894993894993883), (41, 0.93894993894993883)]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796139</td>\n",
       "      <td>0.796139</td>\n",
       "      <td>0.761299</td>\n",
       "      <td>0.761299</td>\n",
       "      <td>0.747175</td>\n",
       "      <td>0.747175</td>\n",
       "      <td>0.813089</td>\n",
       "      <td>0.813089</td>\n",
       "      <td>0.855461</td>\n",
       "      <td>0.855461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930791</td>\n",
       "      <td>0.930791</td>\n",
       "      <td>0.934557</td>\n",
       "      <td>0.934557</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>0.920904</td>\n",
       "      <td>0.923258</td>\n",
       "      <td>0.923258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.745690</td>\n",
       "      <td>0.745690</td>\n",
       "      <td>0.745211</td>\n",
       "      <td>0.745211</td>\n",
       "      <td>0.735153</td>\n",
       "      <td>0.735153</td>\n",
       "      <td>0.838123</td>\n",
       "      <td>0.838123</td>\n",
       "      <td>0.857759</td>\n",
       "      <td>0.857759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934387</td>\n",
       "      <td>0.934387</td>\n",
       "      <td>0.936782</td>\n",
       "      <td>0.936782</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.919540</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.923851</td>\n",
       "      <td>0.923851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.788314</td>\n",
       "      <td>0.788314</td>\n",
       "      <td>0.745211</td>\n",
       "      <td>0.745211</td>\n",
       "      <td>0.744732</td>\n",
       "      <td>0.744732</td>\n",
       "      <td>0.826149</td>\n",
       "      <td>0.826149</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.943008</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.939655</td>\n",
       "      <td>0.912356</td>\n",
       "      <td>0.912356</td>\n",
       "      <td>0.937261</td>\n",
       "      <td>0.937261</td>\n",
       "      <td>0.942050</td>\n",
       "      <td>0.942050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.779744</td>\n",
       "      <td>0.779744</td>\n",
       "      <td>0.757129</td>\n",
       "      <td>0.757129</td>\n",
       "      <td>0.742871</td>\n",
       "      <td>0.742871</td>\n",
       "      <td>0.792527</td>\n",
       "      <td>0.792527</td>\n",
       "      <td>0.820059</td>\n",
       "      <td>0.820059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.942478</td>\n",
       "      <td>0.942478</td>\n",
       "      <td>0.926254</td>\n",
       "      <td>0.926254</td>\n",
       "      <td>0.935595</td>\n",
       "      <td>0.935595</td>\n",
       "      <td>0.925762</td>\n",
       "      <td>0.925762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781346</td>\n",
       "      <td>0.781346</td>\n",
       "      <td>0.751274</td>\n",
       "      <td>0.751274</td>\n",
       "      <td>0.747197</td>\n",
       "      <td>0.747197</td>\n",
       "      <td>0.869521</td>\n",
       "      <td>0.869521</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950051</td>\n",
       "      <td>0.950051</td>\n",
       "      <td>0.946993</td>\n",
       "      <td>0.946993</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.942915</td>\n",
       "      <td>0.942915</td>\n",
       "      <td>0.940877</td>\n",
       "      <td>0.940877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.798546</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>0.769470</td>\n",
       "      <td>0.759605</td>\n",
       "      <td>0.759605</td>\n",
       "      <td>0.821911</td>\n",
       "      <td>0.821911</td>\n",
       "      <td>0.855140</td>\n",
       "      <td>0.855140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953790</td>\n",
       "      <td>0.953790</td>\n",
       "      <td>0.948598</td>\n",
       "      <td>0.948598</td>\n",
       "      <td>0.911734</td>\n",
       "      <td>0.911734</td>\n",
       "      <td>0.923676</td>\n",
       "      <td>0.923676</td>\n",
       "      <td>0.926791</td>\n",
       "      <td>0.926791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.805291</td>\n",
       "      <td>0.805291</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.847090</td>\n",
       "      <td>0.847090</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>0.860317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938624</td>\n",
       "      <td>0.938624</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.948148</td>\n",
       "      <td>0.921693</td>\n",
       "      <td>0.921693</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.939683</td>\n",
       "      <td>0.937037</td>\n",
       "      <td>0.937037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.783654</td>\n",
       "      <td>0.844017</td>\n",
       "      <td>0.844017</td>\n",
       "      <td>0.870726</td>\n",
       "      <td>0.870726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.946581</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.933761</td>\n",
       "      <td>0.933761</td>\n",
       "      <td>0.935897</td>\n",
       "      <td>0.935897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.798274</td>\n",
       "      <td>0.798274</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>0.868393</td>\n",
       "      <td>0.868393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>0.926106</td>\n",
       "      <td>0.926106</td>\n",
       "      <td>0.925566</td>\n",
       "      <td>0.925566</td>\n",
       "      <td>0.929342</td>\n",
       "      <td>0.929342</td>\n",
       "      <td>0.922869</td>\n",
       "      <td>0.922869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.803630</td>\n",
       "      <td>0.803630</td>\n",
       "      <td>0.782728</td>\n",
       "      <td>0.782728</td>\n",
       "      <td>0.764026</td>\n",
       "      <td>0.764026</td>\n",
       "      <td>0.879538</td>\n",
       "      <td>0.879538</td>\n",
       "      <td>0.872387</td>\n",
       "      <td>0.872387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957096</td>\n",
       "      <td>0.957096</td>\n",
       "      <td>0.951045</td>\n",
       "      <td>0.951045</td>\n",
       "      <td>0.937294</td>\n",
       "      <td>0.937294</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.946095</td>\n",
       "      <td>0.946095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.796139  0.796139  0.761299  0.761299  0.747175  0.747175  0.813089   \n",
       "1  0.745690  0.745690  0.745211  0.745211  0.735153  0.735153  0.838123   \n",
       "2  0.788314  0.788314  0.745211  0.745211  0.744732  0.744732  0.826149   \n",
       "3  0.779744  0.779744  0.757129  0.757129  0.742871  0.742871  0.792527   \n",
       "4  0.781346  0.781346  0.751274  0.751274  0.747197  0.747197  0.869521   \n",
       "5  0.798546  0.798546  0.769470  0.769470  0.759605  0.759605  0.821911   \n",
       "6  0.805291  0.805291  0.771429  0.771429  0.766667  0.766667  0.847090   \n",
       "7  0.803419  0.803419  0.788462  0.788462  0.783654  0.783654  0.844017   \n",
       "8  0.798274  0.798274  0.789105  0.789105  0.773463  0.773463  0.855448   \n",
       "9  0.803630  0.803630  0.782728  0.782728  0.764026  0.764026  0.879538   \n",
       "\n",
       "        7         8         9      ...          98        99        100  \\\n",
       "0  0.813089  0.855461  0.855461    ...     0.930791  0.930791  0.934557   \n",
       "1  0.838123  0.857759  0.857759    ...     0.934387  0.934387  0.936782   \n",
       "2  0.826149  0.833333  0.833333    ...     0.943008  0.943008  0.939655   \n",
       "3  0.792527  0.820059  0.820059    ...     0.946903  0.946903  0.942478   \n",
       "4  0.869521  0.871560  0.871560    ...     0.950051  0.950051  0.946993   \n",
       "5  0.821911  0.855140  0.855140    ...     0.953790  0.953790  0.948598   \n",
       "6  0.847090  0.860317  0.860317    ...     0.938624  0.938624  0.948148   \n",
       "7  0.844017  0.870726  0.870726    ...     0.942842  0.942842  0.946581   \n",
       "8  0.855448  0.868393  0.868393    ...     0.930960  0.930960  0.926106   \n",
       "9  0.879538  0.872387  0.872387    ...     0.957096  0.957096  0.951045   \n",
       "\n",
       "        101       102       103       104       105       106       107  \n",
       "0  0.934557  0.916667  0.916667  0.920904  0.920904  0.923258  0.923258  \n",
       "1  0.936782  0.919540  0.919540  0.925287  0.925287  0.923851  0.923851  \n",
       "2  0.939655  0.912356  0.912356  0.937261  0.937261  0.942050  0.942050  \n",
       "3  0.942478  0.926254  0.926254  0.935595  0.935595  0.925762  0.925762  \n",
       "4  0.946993  0.926606  0.926606  0.942915  0.942915  0.940877  0.940877  \n",
       "5  0.948598  0.911734  0.911734  0.923676  0.923676  0.926791  0.926791  \n",
       "6  0.948148  0.921693  0.921693  0.939683  0.939683  0.937037  0.937037  \n",
       "7  0.946581  0.925214  0.925214  0.933761  0.933761  0.935897  0.935897  \n",
       "8  0.926106  0.925566  0.925566  0.929342  0.929342  0.922869  0.922869  \n",
       "9  0.951045  0.937294  0.937294  0.944444  0.944444  0.946095  0.946095  \n",
       "\n",
       "[10 rows x 108 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Classifier: KNeighborsClassifier\n",
      "--------------------------------------------------------------------------------\n",
      "Best score: 0.858312983313\n",
      "Best index: 1 [0 - 17]\n",
      "--------------------------------------------------------------------------------\n",
      "Means: [ 0.81659544  0.85831298  0.83898046  0.85470085  0.84279609  0.8503256\n",
      "  0.81664632  0.85800773  0.83887871  0.8548026   0.84310134  0.85108873\n",
      "  0.81679894  0.85719373  0.83567359  0.85388685  0.84238909  0.85027473]\n",
      "Stds: [ 0.01462143  0.0156129   0.01151133  0.01530145  0.01213505  0.01232171\n",
      "  0.01549183  0.01505218  0.0112015   0.01459152  0.01050521  0.01253393\n",
      "  0.01443898  0.01385064  0.01137132  0.01531033  0.01188337  0.01237564]\n",
      "--------------------------------------------------------------------------------\n",
      "Max mean: ([1], 0.85831298331298334) - Std: [(1, 0.015612897076307699)]\n",
      "Min std: ([10], 0.010505214475383348) - Mean: [(10, 0.8431013431013431)]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842279</td>\n",
       "      <td>0.879002</td>\n",
       "      <td>0.859699</td>\n",
       "      <td>0.878060</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.848870</td>\n",
       "      <td>0.843220</td>\n",
       "      <td>0.875235</td>\n",
       "      <td>0.862524</td>\n",
       "      <td>0.875235</td>\n",
       "      <td>0.848399</td>\n",
       "      <td>0.851224</td>\n",
       "      <td>0.842750</td>\n",
       "      <td>0.874765</td>\n",
       "      <td>0.857345</td>\n",
       "      <td>0.877589</td>\n",
       "      <td>0.846987</td>\n",
       "      <td>0.846987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.811303</td>\n",
       "      <td>0.850096</td>\n",
       "      <td>0.837165</td>\n",
       "      <td>0.834291</td>\n",
       "      <td>0.855843</td>\n",
       "      <td>0.864943</td>\n",
       "      <td>0.812261</td>\n",
       "      <td>0.851054</td>\n",
       "      <td>0.837165</td>\n",
       "      <td>0.834291</td>\n",
       "      <td>0.852490</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>0.812261</td>\n",
       "      <td>0.851054</td>\n",
       "      <td>0.831418</td>\n",
       "      <td>0.833812</td>\n",
       "      <td>0.854406</td>\n",
       "      <td>0.864943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.797893</td>\n",
       "      <td>0.851533</td>\n",
       "      <td>0.840517</td>\n",
       "      <td>0.843870</td>\n",
       "      <td>0.825670</td>\n",
       "      <td>0.825192</td>\n",
       "      <td>0.798851</td>\n",
       "      <td>0.851533</td>\n",
       "      <td>0.842433</td>\n",
       "      <td>0.845307</td>\n",
       "      <td>0.826628</td>\n",
       "      <td>0.825670</td>\n",
       "      <td>0.802203</td>\n",
       "      <td>0.850096</td>\n",
       "      <td>0.842912</td>\n",
       "      <td>0.843870</td>\n",
       "      <td>0.823755</td>\n",
       "      <td>0.825670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797935</td>\n",
       "      <td>0.854474</td>\n",
       "      <td>0.815634</td>\n",
       "      <td>0.828909</td>\n",
       "      <td>0.818584</td>\n",
       "      <td>0.833825</td>\n",
       "      <td>0.795477</td>\n",
       "      <td>0.852999</td>\n",
       "      <td>0.818092</td>\n",
       "      <td>0.830383</td>\n",
       "      <td>0.821534</td>\n",
       "      <td>0.833825</td>\n",
       "      <td>0.794985</td>\n",
       "      <td>0.855949</td>\n",
       "      <td>0.816618</td>\n",
       "      <td>0.830875</td>\n",
       "      <td>0.817601</td>\n",
       "      <td>0.832842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.804791</td>\n",
       "      <td>0.849134</td>\n",
       "      <td>0.838430</td>\n",
       "      <td>0.848624</td>\n",
       "      <td>0.833843</td>\n",
       "      <td>0.851172</td>\n",
       "      <td>0.804791</td>\n",
       "      <td>0.849134</td>\n",
       "      <td>0.841998</td>\n",
       "      <td>0.850153</td>\n",
       "      <td>0.840979</td>\n",
       "      <td>0.852192</td>\n",
       "      <td>0.803772</td>\n",
       "      <td>0.849134</td>\n",
       "      <td>0.823649</td>\n",
       "      <td>0.840979</td>\n",
       "      <td>0.839450</td>\n",
       "      <td>0.851682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.814642</td>\n",
       "      <td>0.871236</td>\n",
       "      <td>0.825545</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>0.851506</td>\n",
       "      <td>0.867601</td>\n",
       "      <td>0.812565</td>\n",
       "      <td>0.871236</td>\n",
       "      <td>0.833853</td>\n",
       "      <td>0.873313</td>\n",
       "      <td>0.853063</td>\n",
       "      <td>0.868120</td>\n",
       "      <td>0.819315</td>\n",
       "      <td>0.869678</td>\n",
       "      <td>0.829699</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>0.847871</td>\n",
       "      <td>0.865524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.835450</td>\n",
       "      <td>0.888360</td>\n",
       "      <td>0.843915</td>\n",
       "      <td>0.858201</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.888360</td>\n",
       "      <td>0.841799</td>\n",
       "      <td>0.856614</td>\n",
       "      <td>0.850265</td>\n",
       "      <td>0.856614</td>\n",
       "      <td>0.834392</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.841799</td>\n",
       "      <td>0.857672</td>\n",
       "      <td>0.848677</td>\n",
       "      <td>0.857672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.813568</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.845620</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.852564</td>\n",
       "      <td>0.854701</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.831197</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.849893</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.838141</td>\n",
       "      <td>0.860043</td>\n",
       "      <td>0.852030</td>\n",
       "      <td>0.856303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.829558</td>\n",
       "      <td>0.852211</td>\n",
       "      <td>0.837109</td>\n",
       "      <td>0.866775</td>\n",
       "      <td>0.847357</td>\n",
       "      <td>0.852211</td>\n",
       "      <td>0.832794</td>\n",
       "      <td>0.853830</td>\n",
       "      <td>0.832255</td>\n",
       "      <td>0.865156</td>\n",
       "      <td>0.842503</td>\n",
       "      <td>0.855448</td>\n",
       "      <td>0.828479</td>\n",
       "      <td>0.851672</td>\n",
       "      <td>0.829558</td>\n",
       "      <td>0.864617</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.854369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.820132</td>\n",
       "      <td>0.854235</td>\n",
       "      <td>0.845985</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.845435</td>\n",
       "      <td>0.852585</td>\n",
       "      <td>0.817932</td>\n",
       "      <td>0.854235</td>\n",
       "      <td>0.845985</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.847085</td>\n",
       "      <td>0.849835</td>\n",
       "      <td>0.817932</td>\n",
       "      <td>0.854235</td>\n",
       "      <td>0.844884</td>\n",
       "      <td>0.861386</td>\n",
       "      <td>0.850385</td>\n",
       "      <td>0.849285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.842279  0.879002  0.859699  0.878060  0.847458  0.848870  0.843220   \n",
       "1  0.811303  0.850096  0.837165  0.834291  0.855843  0.864943  0.812261   \n",
       "2  0.797893  0.851533  0.840517  0.843870  0.825670  0.825192  0.798851   \n",
       "3  0.797935  0.854474  0.815634  0.828909  0.818584  0.833825  0.795477   \n",
       "4  0.804791  0.849134  0.838430  0.848624  0.833843  0.851172  0.804791   \n",
       "5  0.814642  0.871236  0.825545  0.870717  0.851506  0.867601  0.812565   \n",
       "6  0.835450  0.888360  0.843915  0.858201  0.851852  0.854497  0.837037   \n",
       "7  0.813568  0.831731  0.845620  0.858974  0.852564  0.854701  0.813034   \n",
       "8  0.829558  0.852211  0.837109  0.866775  0.847357  0.852211  0.832794   \n",
       "9  0.820132  0.854235  0.845985  0.861386  0.845435  0.852585  0.817932   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.875235  0.862524  0.875235  0.848399  0.851224  0.842750  0.874765   \n",
       "1  0.851054  0.837165  0.834291  0.852490  0.865900  0.812261  0.851054   \n",
       "2  0.851533  0.842433  0.845307  0.826628  0.825670  0.802203  0.850096   \n",
       "3  0.852999  0.818092  0.830383  0.821534  0.833825  0.794985  0.855949   \n",
       "4  0.849134  0.841998  0.850153  0.840979  0.852192  0.803772  0.849134   \n",
       "5  0.871236  0.833853  0.873313  0.853063  0.868120  0.819315  0.869678   \n",
       "6  0.888360  0.841799  0.856614  0.850265  0.856614  0.834392  0.882540   \n",
       "7  0.831731  0.831197  0.858974  0.849893  0.854167  0.813034  0.831731   \n",
       "8  0.853830  0.832255  0.865156  0.842503  0.855448  0.828479  0.851672   \n",
       "9  0.854235  0.845985  0.861386  0.847085  0.849835  0.817932  0.854235   \n",
       "\n",
       "         14        15        16        17  \n",
       "0  0.857345  0.877589  0.846987  0.846987  \n",
       "1  0.831418  0.833812  0.854406  0.864943  \n",
       "2  0.842912  0.843870  0.823755  0.825670  \n",
       "3  0.816618  0.830875  0.817601  0.832842  \n",
       "4  0.823649  0.840979  0.839450  0.851682  \n",
       "5  0.829699  0.870717  0.847871  0.865524  \n",
       "6  0.841799  0.857672  0.848677  0.857672  \n",
       "7  0.838141  0.860043  0.852030  0.856303  \n",
       "8  0.829558  0.864617  0.845200  0.854369  \n",
       "9  0.844884  0.861386  0.850385  0.849285  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Classifier: SVC\n",
      "--------------------------------------------------------------------------------\n",
      "Best score: 0.938186813187\n",
      "Best index: 16 [0 - 31]\n",
      "--------------------------------------------------------------------------------\n",
      "Means: [ 0.93452381  0.93452381  0.71275946  0.71275946  0.71377696  0.71377696\n",
      "  0.71815222  0.71815222  0.93452381  0.93452381  0.71275946  0.71275946\n",
      "  0.71377696  0.71377696  0.71815222  0.71815222  0.93818681  0.93818681\n",
      "  0.74537037  0.74537037  0.75529101  0.75529101  0.68167481  0.68167481\n",
      "  0.93818681  0.93818681  0.74537037  0.74537037  0.75529101  0.75529101\n",
      "  0.68167481  0.68167481]\n",
      "Stds: [ 0.00482109  0.00482109  0.00726976  0.00726976  0.00679318  0.00679318\n",
      "  0.0083594   0.0083594   0.00482109  0.00482109  0.00726976  0.00726976\n",
      "  0.00679318  0.00679318  0.0083594   0.0083594   0.00516983  0.00516983\n",
      "  0.01204597  0.01204597  0.01191897  0.01191897  0.01116556  0.01116556\n",
      "  0.00516983  0.00516983  0.01204597  0.01204597  0.01191897  0.01191897\n",
      "  0.01116556  0.01116556]\n",
      "--------------------------------------------------------------------------------\n",
      "Max mean: ([16, 17, 24, 25], 0.93818681318681318) - Std: [(16, 0.0051698278935005532), (17, 0.0051698278935005532), (24, 0.0051698278935005532), (25, 0.0051698278935005532)]\n",
      "Min std: ([0, 1, 8, 9], 0.0048210853126934008) - Mean: [(0, 0.93452380952380953), (1, 0.93452380952380953), (8, 0.93452380952380953), (9, 0.93452380952380953)]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.935499</td>\n",
       "      <td>0.935499</td>\n",
       "      <td>0.702919</td>\n",
       "      <td>0.702919</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.703390</td>\n",
       "      <td>0.707156</td>\n",
       "      <td>0.707156</td>\n",
       "      <td>0.935499</td>\n",
       "      <td>0.935499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.938795</td>\n",
       "      <td>0.938795</td>\n",
       "      <td>0.736347</td>\n",
       "      <td>0.736347</td>\n",
       "      <td>0.751883</td>\n",
       "      <td>0.751883</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.929119</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>0.705460</td>\n",
       "      <td>0.705460</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.709291</td>\n",
       "      <td>0.709291</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>0.929119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683429</td>\n",
       "      <td>0.683429</td>\n",
       "      <td>0.934387</td>\n",
       "      <td>0.934387</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.748563</td>\n",
       "      <td>0.748563</td>\n",
       "      <td>0.683429</td>\n",
       "      <td>0.683429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.932471</td>\n",
       "      <td>0.932471</td>\n",
       "      <td>0.703544</td>\n",
       "      <td>0.703544</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.707375</td>\n",
       "      <td>0.709291</td>\n",
       "      <td>0.709291</td>\n",
       "      <td>0.932471</td>\n",
       "      <td>0.932471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674808</td>\n",
       "      <td>0.674808</td>\n",
       "      <td>0.933908</td>\n",
       "      <td>0.933908</td>\n",
       "      <td>0.725575</td>\n",
       "      <td>0.725575</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.735632</td>\n",
       "      <td>0.674808</td>\n",
       "      <td>0.674808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.709440</td>\n",
       "      <td>0.710423</td>\n",
       "      <td>0.710423</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>0.929204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670108</td>\n",
       "      <td>0.670108</td>\n",
       "      <td>0.939036</td>\n",
       "      <td>0.939036</td>\n",
       "      <td>0.733038</td>\n",
       "      <td>0.733038</td>\n",
       "      <td>0.735497</td>\n",
       "      <td>0.735497</td>\n",
       "      <td>0.670108</td>\n",
       "      <td>0.670108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.710499</td>\n",
       "      <td>0.710499</td>\n",
       "      <td>0.712029</td>\n",
       "      <td>0.712029</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673293</td>\n",
       "      <td>0.673293</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.753313</td>\n",
       "      <td>0.753313</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.758410</td>\n",
       "      <td>0.673293</td>\n",
       "      <td>0.673293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.934060</td>\n",
       "      <td>0.934060</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>0.719107</td>\n",
       "      <td>0.719107</td>\n",
       "      <td>0.723261</td>\n",
       "      <td>0.723261</td>\n",
       "      <td>0.934060</td>\n",
       "      <td>0.934060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687954</td>\n",
       "      <td>0.687954</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>0.746106</td>\n",
       "      <td>0.746106</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.687954</td>\n",
       "      <td>0.687954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.719577</td>\n",
       "      <td>0.719577</td>\n",
       "      <td>0.722751</td>\n",
       "      <td>0.722751</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.724868</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684656</td>\n",
       "      <td>0.684656</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.747090</td>\n",
       "      <td>0.747090</td>\n",
       "      <td>0.764021</td>\n",
       "      <td>0.764021</td>\n",
       "      <td>0.684656</td>\n",
       "      <td>0.684656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.727564</td>\n",
       "      <td>0.727564</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>0.931624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.746795</td>\n",
       "      <td>0.746795</td>\n",
       "      <td>0.767628</td>\n",
       "      <td>0.767628</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.721683</td>\n",
       "      <td>0.721683</td>\n",
       "      <td>0.721683</td>\n",
       "      <td>0.721683</td>\n",
       "      <td>0.725998</td>\n",
       "      <td>0.725998</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>0.932039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690399</td>\n",
       "      <td>0.690399</td>\n",
       "      <td>0.936893</td>\n",
       "      <td>0.936893</td>\n",
       "      <td>0.766451</td>\n",
       "      <td>0.766451</td>\n",
       "      <td>0.768069</td>\n",
       "      <td>0.768069</td>\n",
       "      <td>0.690399</td>\n",
       "      <td>0.690399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.936194</td>\n",
       "      <td>0.936194</td>\n",
       "      <td>0.719472</td>\n",
       "      <td>0.719472</td>\n",
       "      <td>0.715622</td>\n",
       "      <td>0.715622</td>\n",
       "      <td>0.729923</td>\n",
       "      <td>0.729923</td>\n",
       "      <td>0.936194</td>\n",
       "      <td>0.936194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701870</td>\n",
       "      <td>0.701870</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.949395</td>\n",
       "      <td>0.762926</td>\n",
       "      <td>0.762926</td>\n",
       "      <td>0.766777</td>\n",
       "      <td>0.766777</td>\n",
       "      <td>0.701870</td>\n",
       "      <td>0.701870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.935499  0.935499  0.702919  0.702919  0.703390  0.703390  0.707156   \n",
       "1  0.929119  0.929119  0.705460  0.705460  0.707375  0.707375  0.709291   \n",
       "2  0.932471  0.932471  0.703544  0.703544  0.707375  0.707375  0.709291   \n",
       "3  0.929204  0.929204  0.709440  0.709440  0.709440  0.709440  0.710423   \n",
       "4  0.940367  0.940367  0.710499  0.710499  0.712029  0.712029  0.718145   \n",
       "5  0.934060  0.934060  0.716511  0.716511  0.719107  0.719107  0.723261   \n",
       "6  0.945503  0.945503  0.719577  0.719577  0.722751  0.722751  0.724868   \n",
       "7  0.931624  0.931624  0.722222  0.722222  0.722222  0.722222  0.727564   \n",
       "8  0.932039  0.932039  0.721683  0.721683  0.721683  0.721683  0.725998   \n",
       "9  0.936194  0.936194  0.719472  0.719472  0.715622  0.715622  0.729923   \n",
       "\n",
       "         7         8         9     ...           22        23        24  \\\n",
       "0  0.707156  0.935499  0.935499    ...     0.662900  0.662900  0.938795   \n",
       "1  0.709291  0.929119  0.929119    ...     0.683429  0.683429  0.934387   \n",
       "2  0.709291  0.932471  0.932471    ...     0.674808  0.674808  0.933908   \n",
       "3  0.710423  0.929204  0.929204    ...     0.670108  0.670108  0.939036   \n",
       "4  0.718145  0.940367  0.940367    ...     0.673293  0.673293  0.944444   \n",
       "5  0.723261  0.934060  0.934060    ...     0.687954  0.687954  0.934579   \n",
       "6  0.724868  0.945503  0.945503    ...     0.684656  0.684656  0.940741   \n",
       "7  0.727564  0.931624  0.931624    ...     0.692308  0.692308  0.930556   \n",
       "8  0.725998  0.932039  0.932039    ...     0.690399  0.690399  0.936893   \n",
       "9  0.729923  0.936194  0.936194    ...     0.701870  0.701870  0.949395   \n",
       "\n",
       "         25        26        27        28        29        30        31  \n",
       "0  0.938795  0.736347  0.736347  0.751883  0.751883  0.662900  0.662900  \n",
       "1  0.934387  0.741379  0.741379  0.748563  0.748563  0.683429  0.683429  \n",
       "2  0.933908  0.725575  0.725575  0.735632  0.735632  0.674808  0.674808  \n",
       "3  0.939036  0.733038  0.733038  0.735497  0.735497  0.670108  0.670108  \n",
       "4  0.944444  0.753313  0.753313  0.758410  0.758410  0.673293  0.673293  \n",
       "5  0.934579  0.746106  0.746106  0.761682  0.761682  0.687954  0.687954  \n",
       "6  0.940741  0.747090  0.747090  0.764021  0.764021  0.684656  0.684656  \n",
       "7  0.930556  0.746795  0.746795  0.767628  0.767628  0.692308  0.692308  \n",
       "8  0.936893  0.766451  0.766451  0.768069  0.768069  0.690399  0.690399  \n",
       "9  0.949395  0.762926  0.762926  0.766777  0.766777  0.701870  0.701870  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Classifier: LinearSVC\n",
      "--------------------------------------------------------------------------------\n",
      "Best score: 0.938542938543\n",
      "Best index: 3 [0 - 3]\n",
      "--------------------------------------------------------------------------------\n",
      "Means: [ 0.92114367  0.93162393  0.93711844  0.93854294]\n",
      "Stds: [ 0.01246695  0.00654971  0.00585091  0.00637784]\n",
      "--------------------------------------------------------------------------------\n",
      "Max mean: ([3], 0.93854293854293858) - Std: [(3, 0.0063778385811532057)]\n",
      "Min std: ([2], 0.0058509128016236136) - Mean: [(2, 0.93711843711843701)]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.915725</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.935970</td>\n",
       "      <td>0.939736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.909004</td>\n",
       "      <td>0.920019</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.926245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.904693</td>\n",
       "      <td>0.923851</td>\n",
       "      <td>0.938218</td>\n",
       "      <td>0.936782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.917896</td>\n",
       "      <td>0.933137</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.936087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.925586</td>\n",
       "      <td>0.935270</td>\n",
       "      <td>0.931193</td>\n",
       "      <td>0.931702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.928349</td>\n",
       "      <td>0.935099</td>\n",
       "      <td>0.943406</td>\n",
       "      <td>0.940291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.945503</td>\n",
       "      <td>0.943386</td>\n",
       "      <td>0.948677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.936966</td>\n",
       "      <td>0.928953</td>\n",
       "      <td>0.932158</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.910464</td>\n",
       "      <td>0.930960</td>\n",
       "      <td>0.942826</td>\n",
       "      <td>0.946063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.919142</td>\n",
       "      <td>0.932893</td>\n",
       "      <td>0.942244</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.915725  0.932203  0.935970  0.939736\n",
       "1  0.909004  0.920019  0.925287  0.926245\n",
       "2  0.904693  0.923851  0.938218  0.936782\n",
       "3  0.917896  0.933137  0.938053  0.936087\n",
       "4  0.925586  0.935270  0.931193  0.931702\n",
       "5  0.928349  0.935099  0.943406  0.940291\n",
       "6  0.947090  0.945503  0.943386  0.948677\n",
       "7  0.936966  0.928953  0.932158  0.937500\n",
       "8  0.910464  0.930960  0.942826  0.946063\n",
       "9  0.919142  0.932893  0.942244  0.944444"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for clf_name in results:\n",
    "    print('-' * 80)\n",
    "    print('Classifier:', clf_name)\n",
    "    print('-' * 80)\n",
    "    grid_search_cv = results[clf_name]\n",
    "    n_opts = len(grid_search_cv.cv_results_['rank_test_score'])\n",
    "    print('Best score:', grid_search_cv.best_score_)\n",
    "    print('Best index:', grid_search_cv.best_index_, '[0 - ' + str(n_opts - 1) + ']')\n",
    "    print('-' * 80)\n",
    "    means = grid_search_cv.cv_results_['mean_test_score']\n",
    "    stds = grid_search_cv.cv_results_['std_test_score']\n",
    "    print('Means:', means)\n",
    "    print('Stds:', stds)\n",
    "    print('-' * 80)\n",
    "    max_means = max_index_value(means)\n",
    "    min_stds = min_index_value(stds)\n",
    "    print('Max mean:', max_means, '- Std:', [(i, stds[i]) for i in max_means[0]])\n",
    "    print('Min std:', min_stds, '- Mean:', [(i, means[i]) for i in min_stds[0]])\n",
    "    print('-' * 80)\n",
    "    result_df = pd.DataFrame(\n",
    "        [grid_search_cv.cv_results_['split' + str(i) + '_test_score'] for i in range(grid_search_cv.n_splits_)],\n",
    "    )\n",
    "    display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_cv = results['RandomForestClassifier']\n",
    "best_estimator = grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'random_state': 0,\n",
       " 'warm_start': True}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  1.35362220e-01,   1.31701541e-01,   3.38694358e-01,\n",
       "          3.15328646e-01,   2.75847230e+00,   2.89844530e+00,\n",
       "          1.31958127e-01,   1.37901211e-01,   4.41786289e-01,\n",
       "          4.66601968e-01,   5.10951388e+00,   5.28257387e+00,\n",
       "          8.52949023e-01,   6.77123737e-01,   8.64378912e+00,\n",
       "          9.68664005e+00,   1.04791235e+02,   1.03196397e+02,\n",
       "          2.77846861e-01,   1.95573044e-01,   5.42894053e-01,\n",
       "          5.16217971e-01,   5.27265134e+00,   5.42738438e+00,\n",
       "          1.73891187e-01,   1.44829822e-01,   1.06491604e+00,\n",
       "          1.14952736e+00,   1.11739807e+01,   1.12098390e+01,\n",
       "          1.59857678e+00,   1.39608605e+00,   1.56050876e+01,\n",
       "          1.55422227e+01,   1.55624943e+02,   1.48978384e+02,\n",
       "          2.94592094e-01,   1.58010268e-01,   7.84864068e-01,\n",
       "          8.50752497e-01,   8.69089534e+00,   8.44566281e+00,\n",
       "          2.13960361e-01,   1.40977597e-01,   1.42526162e+00,\n",
       "          1.37072380e+00,   1.47418383e+01,   1.50284375e+01,\n",
       "          1.56997390e+00,   1.58191040e+00,   1.63537228e+01,\n",
       "          1.60760187e+01,   1.62571557e+02,   1.42321220e+02,\n",
       "          2.19557524e-01,   1.32858968e-01,   3.05632138e-01,\n",
       "          3.04578710e-01,   2.73374519e+00,   2.79078331e+00,\n",
       "          1.36786127e-01,   1.30247784e-01,   5.03258061e-01,\n",
       "          4.73577762e-01,   5.18972094e+00,   5.88922391e+00,\n",
       "          7.59605789e-01,   6.67750764e-01,   8.53682573e+00,\n",
       "          8.52645361e+00,   8.70232785e+01,   8.86512787e+01,\n",
       "          2.45896864e-01,   1.32352996e-01,   3.77779913e-01,\n",
       "          4.01828814e-01,   4.22618990e+00,   4.36950884e+00,\n",
       "          1.56155086e-01,   1.37551475e-01,   9.01788211e-01,\n",
       "          8.74905014e-01,   9.57156560e+00,   9.79809396e+00,\n",
       "          1.19886737e+00,   1.02353282e+00,   1.22766268e+01,\n",
       "          1.22080551e+01,   1.27178753e+02,   1.35711122e+02,\n",
       "          2.56023216e-01,   1.77831388e-01,   7.47146440e-01,\n",
       "          7.44773817e-01,   8.58417449e+00,   8.53562253e+00,\n",
       "          1.83183193e-01,   1.86355376e-01,   1.39168892e+00,\n",
       "          1.41258354e+00,   1.32840996e+01,   1.31649417e+01,\n",
       "          1.44547143e+00,   1.20307715e+00,   1.38223539e+01,\n",
       "          1.38469900e+01,   1.36887274e+02,   1.36316012e+02]),\n",
       " 'mean_score_time': array([ 0.10361223,  0.10380743,  0.10319695,  0.10325818,  0.20342953,\n",
       "         0.24445293,  0.10280256,  0.10330775,  0.10941629,  0.10778215,\n",
       "         0.25219419,  0.25229168,  0.1270215 ,  0.12445807,  0.14849503,\n",
       "         0.14408989,  0.40394604,  0.39618728,  0.14694395,  0.12575078,\n",
       "         0.1049974 ,  0.1096791 ,  0.35439932,  0.34626207,  0.10848448,\n",
       "         0.1136445 ,  0.11998432,  0.11995151,  0.36558101,  0.42457147,\n",
       "         0.13858933,  0.15939646,  0.15277715,  0.13897772,  0.36717381,\n",
       "         0.45741956,  0.14313326,  0.11163666,  0.11281028,  0.11493192,\n",
       "         0.41351659,  0.36625416,  0.11300204,  0.10663617,  0.12105265,\n",
       "         0.12100477,  0.37415509,  0.42735982,  0.14336283,  0.12669332,\n",
       "         0.15978436,  0.16298811,  0.44739168,  0.35966675,  0.13719265,\n",
       "         0.10394006,  0.10530214,  0.10451248,  0.21895726,  0.22615955,\n",
       "         0.10269561,  0.10415335,  0.10983925,  0.10728564,  0.24034448,\n",
       "         0.30932944,  0.12468834,  0.13536489,  0.14889951,  0.12670569,\n",
       "         0.35608561,  0.35405028,  0.14007666,  0.10513623,  0.10732217,\n",
       "         0.10501883,  0.29687757,  0.29025068,  0.10752718,  0.10507438,\n",
       "         0.1144104 ,  0.1146313 ,  0.33890579,  0.3182348 ,  0.13564312,\n",
       "         0.13090651,  0.14189148,  0.14923472,  0.35473671,  0.38082438,\n",
       "         0.1488364 ,  0.11501696,  0.11823735,  0.11422234,  0.38099618,\n",
       "         0.36053391,  0.11586215,  0.11052198,  0.1226125 ,  0.1289485 ,\n",
       "         0.33015807,  0.35318415,  0.13261049,  0.14088182,  0.12909913,\n",
       "         0.15823436,  0.35742548,  0.35462554]),\n",
       " 'mean_test_score': array([ 0.78947904,  0.78947904,  0.76541514,  0.76541514,  0.75579976,\n",
       "         0.75579976,  0.83786121,  0.83786121,  0.85597273,  0.85597273,\n",
       "         0.85933048,  0.85933048,  0.89255189,  0.89255189,  0.89718152,\n",
       "         0.89718152,  0.89707977,  0.89707977,  0.85129223,  0.85129223,\n",
       "         0.85907611,  0.85907611,  0.85927961,  0.85927961,  0.89397639,\n",
       "         0.89397639,  0.90532153,  0.90532153,  0.90577941,  0.90577941,\n",
       "         0.91381766,  0.91381766,  0.92094017,  0.92094017,  0.9231278 ,\n",
       "         0.9231278 ,  0.88298738,  0.88298738,  0.93264143,  0.93264143,\n",
       "         0.93894994,  0.93894994,  0.91442816,  0.91442816,  0.93579569,\n",
       "         0.93579569,  0.93935694,  0.93935694,  0.91778592,  0.91778592,\n",
       "         0.92531543,  0.92531543,  0.92791005,  0.92791005,  0.83867521,\n",
       "         0.83867521,  0.85871998,  0.85871998,  0.85332723,  0.85332723,\n",
       "         0.88771876,  0.88771876,  0.90135328,  0.90135328,  0.89631665,\n",
       "         0.89631665,  0.91397029,  0.91397029,  0.91625967,  0.91625967,\n",
       "         0.91575092,  0.91575092,  0.8707265 ,  0.8707265 ,  0.90120065,\n",
       "         0.90120065,  0.91168091,  0.91168091,  0.90867928,  0.90867928,\n",
       "         0.93259056,  0.93259056,  0.93569394,  0.93569394,  0.92567155,\n",
       "         0.92567155,  0.92775743,  0.92775743,  0.93147131,  0.93147131,\n",
       "         0.88873626,  0.88873626,  0.93335368,  0.93335368,  0.93910256,\n",
       "         0.93910256,  0.91244404,  0.91244404,  0.94266382,  0.94266382,\n",
       "         0.94195157,  0.94195157,  0.92205942,  0.92205942,  0.93309931,\n",
       "         0.93309931,  0.93228531,  0.93228531]),\n",
       " 'mean_train_score': array([ 0.82456075,  0.82456075,  0.80252294,  0.80252294,  0.79151468,\n",
       "         0.79151468,  0.8742634 ,  0.8742634 ,  0.89274885,  0.89274885,\n",
       "         0.89665102,  0.89665102,  0.93892764,  0.93892764,  0.94369134,\n",
       "         0.94369134,  0.94460196,  0.94460196,  0.94345732,  0.94345732,\n",
       "         0.97551885,  0.97551885,  0.9806886 ,  0.9806886 ,  0.97800364,\n",
       "         0.97800364,  0.99603937,  0.99603937,  0.99752613,  0.99752613,\n",
       "         0.99519613,  0.99519613,  0.99990371,  0.99990371,  0.99994934,\n",
       "         0.99994934,  0.99935587,  0.99935587,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.999407  ,  0.999407  ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99912343,  0.99912343,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89154434,\n",
       "         0.89154434,  0.93098278,  0.93098278,  0.93529988,  0.93529988,\n",
       "         0.93994999,  0.93994999,  0.96644964,  0.96644964,  0.96989927,\n",
       "         0.96989927,  0.96113871,  0.96113871,  0.96991636,  0.96991636,\n",
       "         0.9716681 ,  0.9716681 ,  0.97051752,  0.97051752,  0.99624219,\n",
       "         0.99624219,  0.99829178,  0.99829178,  0.99037401,  0.99037401,\n",
       "         0.99967156,  0.99967156,  1.        ,  1.        ,  0.99624012,\n",
       "         0.99624576,  0.99990425,  0.99990425,  0.99995515,  0.99995515,\n",
       "         0.99945794,  0.99945794,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99944578,  0.99944578,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99906703,  0.99906703,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'param_class_weight': masked_array(data = [ {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  {1987: 0.01282051282051282, 1988: 0.021062271062271064, 1989: 0.022893772893772892, 1990: 0.026556776556776556, 1991: 0.046703296703296704, 1992: 0.031135531135531136, 1993: 0.024725274725274724, 1994: 0.049450549450549448, 1995: 0.042124542124542128, 1996: 0.045787545787545784, 1997: 0.050366300366300368, 1998: 0.06043956043956044, 1999: 0.072344322344322351, 2000: 0.083333333333333329, 2001: 0.089743589743589744, 2003: 0.11355311355311355, 2004: 0.10347985347985347, 2005: 0.10347985347985347}\n",
       "  None None None None None None None None None None None None None None None\n",
       "  None None None None None None None None None None None None None None None\n",
       "  None None None None None None None None None None None None None None None\n",
       "  None None None None None None None None None],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_max_depth': masked_array(data = [10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 20 20 20 20 20 20 20\n",
       "  20 20 20 20 20 20 20 20 20 20 20 None None None None None None None None\n",
       "  None None None None None None None None None None 10 10 10 10 10 10 10 10\n",
       "  10 10 10 10 10 10 10 10 10 10 20 20 20 20 20 20 20 20 20 20 20 20 20 20 20\n",
       "  20 20 20 None None None None None None None None None None None None None\n",
       "  None None None None None],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_max_features': masked_array(data = [10 10 10 10 10 10 'auto' 'auto' 'auto' 'auto' 'auto' 'auto' None None None\n",
       "  None None None 10 10 10 10 10 10 'auto' 'auto' 'auto' 'auto' 'auto' 'auto'\n",
       "  None None None None None None 10 10 10 10 10 10 'auto' 'auto' 'auto'\n",
       "  'auto' 'auto' 'auto' None None None None None None 10 10 10 10 10 10\n",
       "  'auto' 'auto' 'auto' 'auto' 'auto' 'auto' None None None None None None 10\n",
       "  10 10 10 10 10 'auto' 'auto' 'auto' 'auto' 'auto' 'auto' None None None\n",
       "  None None None 10 10 10 10 10 10 'auto' 'auto' 'auto' 'auto' 'auto' 'auto'\n",
       "  None None None None None None],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_estimators': masked_array(data = [10 10 100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000 10\n",
       "  10 100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000 10 10\n",
       "  100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000 10 10\n",
       "  100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000 10 10\n",
       "  100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000 10 10\n",
       "  100 100 1000 1000 10 10 100 100 1000 1000 10 10 100 100 1000 1000],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_n_jobs': masked_array(data = [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
       "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
       "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
       "  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
       "  -1 -1 -1 -1 -1 -1 -1 -1],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_random_state': masked_array(data = [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_warm_start': masked_array(data = [True False True False True False True False True False True False True\n",
       "  False True False True False True False True False True False True False\n",
       "  True False True False True False True False True False True False True\n",
       "  False True False True False True False True False True False True False\n",
       "  True False True False True False True False True False True False True\n",
       "  False True False True False True False True False True False True False\n",
       "  True False True False True False True False True False True False True\n",
       "  False True False True False True False True False True False True False\n",
       "  True False True False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': {1987: 0.01282051282051282,\n",
       "    1988: 0.021062271062271064,\n",
       "    1989: 0.022893772893772892,\n",
       "    1990: 0.026556776556776556,\n",
       "    1991: 0.046703296703296704,\n",
       "    1992: 0.031135531135531136,\n",
       "    1993: 0.024725274725274724,\n",
       "    1994: 0.049450549450549448,\n",
       "    1995: 0.042124542124542128,\n",
       "    1996: 0.045787545787545784,\n",
       "    1997: 0.050366300366300368,\n",
       "    1998: 0.06043956043956044,\n",
       "    1999: 0.072344322344322351,\n",
       "    2000: 0.083333333333333329,\n",
       "    2001: 0.089743589743589744,\n",
       "    2003: 0.11355311355311355,\n",
       "    2004: 0.10347985347985347,\n",
       "    2005: 0.10347985347985347},\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 10,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': 20,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 10,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': 'auto',\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 10,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 100,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': True},\n",
       "  {'class_weight': None,\n",
       "   'max_depth': None,\n",
       "   'max_features': None,\n",
       "   'n_estimators': 1000,\n",
       "   'n_jobs': -1,\n",
       "   'random_state': 0,\n",
       "   'warm_start': False}],\n",
       " 'rank_test_score': array([103, 103, 105, 105, 107, 107, 101, 101,  93,  93,  85,  85,  75,\n",
       "         75,  67,  67,  69,  69,  97,  97,  89,  89,  87,  87,  73,  73,\n",
       "         61,  61,  59,  59,  51,  51,  39,  39,  35,  35,  81,  81,  19,\n",
       "         19,   9,   9,  47,  47,  11,  11,   5,   5,  41,  41,  33,  33,\n",
       "         27,  27,  99,  99,  91,  91,  95,  95,  79,  79,  63,  63,  71,\n",
       "         71,  49,  49,  43,  43,  45,  45,  83,  83,  65,  65,  55,  55,\n",
       "         57,  57,  21,  21,  13,  13,  31,  31,  29,  29,  25,  25,  77,\n",
       "         77,  15,  15,   7,   7,  53,  53,   1,   1,   3,   3,  37,  37,\n",
       "         17,  17,  23,  23], dtype=int32),\n",
       " 'split0_test_score': array([ 0.79613936,  0.79613936,  0.76129944,  0.76129944,  0.74717514,\n",
       "         0.74717514,  0.81308851,  0.81308851,  0.85546139,  0.85546139,\n",
       "         0.86393597,  0.86393597,  0.89077213,  0.89077213,  0.88559322,\n",
       "         0.88559322,  0.88747646,  0.88747646,  0.83945386,  0.83945386,\n",
       "         0.86252354,  0.86252354,  0.85499058,  0.85499058,  0.88182674,\n",
       "         0.88182674,  0.89830508,  0.89830508,  0.90960452,  0.90960452,\n",
       "         0.90160075,  0.90160075,  0.91290019,  0.91290019,  0.91572505,\n",
       "         0.91572505,  0.87947269,  0.87947269,  0.93126177,  0.93126177,\n",
       "         0.94397363,  0.94397363,  0.91242938,  0.91242938,  0.93032015,\n",
       "         0.93032015,  0.93549906,  0.93549906,  0.91148776,  0.91148776,\n",
       "         0.91478343,  0.91478343,  0.91902072,  0.91902072,  0.84369115,\n",
       "         0.84369115,  0.85734463,  0.85734463,  0.8460452 ,  0.8460452 ,\n",
       "         0.88653484,  0.88653484,  0.91290019,  0.91290019,  0.89312618,\n",
       "         0.89312618,  0.8992467 ,  0.8992467 ,  0.90301318,  0.90301318,\n",
       "         0.9039548 ,  0.9039548 ,  0.85169492,  0.85169492,  0.91101695,\n",
       "         0.91101695,  0.91431262,  0.91431262,  0.89500942,  0.89500942,\n",
       "         0.92937853,  0.92937853,  0.93549906,  0.93549906,  0.92278719,\n",
       "         0.92278719,  0.92090395,  0.92090395,  0.92372881,  0.92372881,\n",
       "         0.88653484,  0.88653484,  0.93173258,  0.93173258,  0.93785311,\n",
       "         0.93785311,  0.91713748,  0.91713748,  0.93079096,  0.93079096,\n",
       "         0.93455744,  0.93455744,  0.91666667,  0.91666667,  0.92090395,\n",
       "         0.92090395,  0.923258  ,  0.923258  ]),\n",
       " 'split0_train_score': array([ 0.83247775,  0.83247775,  0.80663929,  0.80663929,  0.79061145,\n",
       "         0.79061145,  0.85198494,  0.85198494,  0.88717773,  0.88717773,\n",
       "         0.89539128,  0.89539128,  0.93885467,  0.93885467,  0.93856947,\n",
       "         0.93856947,  0.93931097,  0.93931097,  0.93309377,  0.93309377,\n",
       "         0.97553046,  0.97553046,  0.97980835,  0.97980835,  0.97399042,\n",
       "         0.97399042,  0.9937828 ,  0.9937828 ,  0.99760438,  0.99760438,\n",
       "         0.99475245,  0.99475245,  0.99954369,  0.99954369,  1.        ,\n",
       "         1.        ,  0.99942961,  0.99942961,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99960073,  0.99960073,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99903034,  0.99903034,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89721652,\n",
       "         0.89721652,  0.93628793,  0.93628793,  0.93680128,  0.93680128,\n",
       "         0.94005248,  0.94005248,  0.96269678,  0.96269678,  0.97051107,\n",
       "         0.97051107,  0.94598449,  0.94598449,  0.96189824,  0.96189824,\n",
       "         0.9665754 ,  0.9665754 ,  0.96640429,  0.96640429,  0.99686288,\n",
       "         0.99686288,  0.9974903 ,  0.9974903 ,  0.98494182,  0.98494182,\n",
       "         0.99903034,  0.99903034,  1.        ,  1.        ,  0.99075975,\n",
       "         0.99075975,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99971481,  0.99971481,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99891627,  0.99891627,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.998517  ,  0.998517  ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split1_test_score': array([ 0.74568966,  0.74568966,  0.74521073,  0.74521073,  0.73515326,\n",
       "         0.73515326,  0.83812261,  0.83812261,  0.85775862,  0.85775862,\n",
       "         0.85775862,  0.85775862,  0.90277778,  0.90277778,  0.88266284,\n",
       "         0.88266284,  0.88505747,  0.88505747,  0.83908046,  0.83908046,\n",
       "         0.84434866,  0.84434866,  0.84961686,  0.84961686,  0.90373563,\n",
       "         0.90373563,  0.89894636,  0.89894636,  0.90181992,  0.90181992,\n",
       "         0.9085249 ,  0.9085249 ,  0.9085249 ,  0.9085249 ,  0.91427203,\n",
       "         0.91427203,  0.87739464,  0.87739464,  0.93630268,  0.93630268,\n",
       "         0.93438697,  0.93438697,  0.90756705,  0.90756705,  0.92816092,\n",
       "         0.92816092,  0.9295977 ,  0.9295977 ,  0.90900383,  0.90900383,\n",
       "         0.9085249 ,  0.9085249 ,  0.91954023,  0.91954023,  0.83860153,\n",
       "         0.83860153,  0.87021073,  0.87021073,  0.85775862,  0.85775862,\n",
       "         0.89846743,  0.89846743,  0.90038314,  0.90038314,  0.89463602,\n",
       "         0.89463602,  0.9137931 ,  0.9137931 ,  0.89750958,  0.89750958,\n",
       "         0.90086207,  0.90086207,  0.87116858,  0.87116858,  0.90229885,\n",
       "         0.90229885,  0.90948276,  0.90948276,  0.91091954,  0.91091954,\n",
       "         0.92480843,  0.92480843,  0.92816092,  0.92816092,  0.92145594,\n",
       "         0.92145594,  0.91666667,  0.91666667,  0.92193487,  0.92193487,\n",
       "         0.86302682,  0.86302682,  0.9243295 ,  0.9243295 ,  0.93295019,\n",
       "         0.93295019,  0.91427203,  0.91427203,  0.93438697,  0.93438697,\n",
       "         0.93678161,  0.93678161,  0.91954023,  0.91954023,  0.92528736,\n",
       "         0.92528736,  0.92385057,  0.92385057]),\n",
       " 'split1_train_score': array([ 0.81921676,  0.81921676,  0.80811703,  0.80811703,  0.79513889,\n",
       "         0.79513889,  0.87272313,  0.87272313,  0.89594718,  0.89594718,\n",
       "         0.89947632,  0.89947632,  0.94393215,  0.94393215,  0.94552596,\n",
       "         0.94552596,  0.94615209,  0.94615209,  0.94495674,  0.94495674,\n",
       "         0.97882514,  0.97882514,  0.97984973,  0.97984973,  0.98497268,\n",
       "         0.98497268,  0.99806466,  0.99806466,  0.99863388,  0.99863388,\n",
       "         0.99282787,  0.99282787,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99891849,  0.99891849,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99960155,  0.99960155,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99914617,  0.99914617,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.8831398 ,\n",
       "         0.8831398 ,  0.93681694,  0.93681694,  0.93721539,  0.93721539,\n",
       "         0.94751821,  0.94751821,  0.96362705,  0.96362705,  0.96909153,\n",
       "         0.96909153,  0.96317168,  0.96317168,  0.96840847,  0.96840847,\n",
       "         0.97176685,  0.97176685,  0.96840847,  0.96840847,  0.99698315,\n",
       "         0.99698315,  0.99806466,  0.99806466,  0.99066485,  0.99066485,\n",
       "         0.99994308,  0.99994308,  1.        ,  1.        ,  0.99715392,\n",
       "         0.99715392,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.9994877 ,  0.9994877 ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99988616,  0.99988616,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99943078,  0.99943078,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split2_test_score': array([ 0.78831418,  0.78831418,  0.74521073,  0.74521073,  0.7447318 ,\n",
       "         0.7447318 ,  0.82614943,  0.82614943,  0.83333333,  0.83333333,\n",
       "         0.83524904,  0.83524904,  0.86685824,  0.86685824,  0.8908046 ,\n",
       "         0.8908046 ,  0.88984674,  0.88984674,  0.81800766,  0.81800766,\n",
       "         0.83668582,  0.83668582,  0.83764368,  0.83764368,  0.88793103,\n",
       "         0.88793103,  0.89032567,  0.89032567,  0.89128352,  0.89128352,\n",
       "         0.8960728 ,  0.8960728 ,  0.91618774,  0.91618774,  0.91858238,\n",
       "         0.91858238,  0.875     ,  0.875     ,  0.92097701,  0.92097701,\n",
       "         0.93726054,  0.93726054,  0.91762452,  0.91762452,  0.93582375,\n",
       "         0.93582375,  0.93965517,  0.93965517,  0.9085249 ,  0.9085249 ,\n",
       "         0.9243295 ,  0.9243295 ,  0.93151341,  0.93151341,  0.83141762,\n",
       "         0.83141762,  0.83668582,  0.83668582,  0.8256705 ,  0.8256705 ,\n",
       "         0.8591954 ,  0.8591954 ,  0.87547893,  0.87547893,  0.87931034,\n",
       "         0.87931034,  0.90613027,  0.90613027,  0.92193487,  0.92193487,\n",
       "         0.91762452,  0.91762452,  0.86015326,  0.86015326,  0.87835249,\n",
       "         0.87835249,  0.90181992,  0.90181992,  0.90421456,  0.90421456,\n",
       "         0.92816092,  0.92816092,  0.93630268,  0.93630268,  0.91954023,\n",
       "         0.91954023,  0.9295977 ,  0.9295977 ,  0.93869732,  0.93869732,\n",
       "         0.88649425,  0.88649425,  0.90900383,  0.90900383,  0.93438697,\n",
       "         0.93438697,  0.91618774,  0.91618774,  0.94300766,  0.94300766,\n",
       "         0.93965517,  0.93965517,  0.91235632,  0.91235632,  0.93726054,\n",
       "         0.93726054,  0.94204981,  0.94204981]),\n",
       " 'split2_train_score': array([ 0.83754554,  0.83754554,  0.79992031,  0.79992031,  0.79422814,\n",
       "         0.79422814,  0.85934654,  0.85934654,  0.89293033,  0.89293033,\n",
       "         0.90300546,  0.90300546,  0.93977687,  0.93977687,  0.94888434,\n",
       "         0.94888434,  0.95315346,  0.95315346,  0.94734745,  0.94734745,\n",
       "         0.97290528,  0.97290528,  0.98195583,  0.98195583,  0.96579007,\n",
       "         0.96579007,  0.99669854,  0.99669854,  0.99641393,  0.99641393,\n",
       "         0.99556011,  0.99556011,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99971539,  0.99971539,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99926002,  0.99926002,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.9986908 ,  0.9986908 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89754098,\n",
       "         0.89754098,  0.9272541 ,  0.9272541 ,  0.93693078,  0.93693078,\n",
       "         0.92287113,  0.92287113,  0.96135018,  0.96135018,  0.96778233,\n",
       "         0.96778233,  0.95969945,  0.95969945,  0.97227914,  0.97227914,\n",
       "         0.9729622 ,  0.9729622 ,  0.97523907,  0.97523907,  0.99795082,\n",
       "         0.99795082,  0.9986908 ,  0.9986908 ,  0.98986794,  0.98986794,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99675546,\n",
       "         0.99675546,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99926002,  0.99926002,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99971539,  0.99971539,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9992031 ,  0.9992031 ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split3_test_score': array([ 0.77974435,  0.77974435,  0.75712881,  0.75712881,  0.74287119,\n",
       "         0.74287119,  0.79252704,  0.79252704,  0.820059  ,  0.820059  ,\n",
       "         0.83333333,  0.83333333,  0.8972468 ,  0.8972468 ,  0.89183874,\n",
       "         0.89183874,  0.8913471 ,  0.8913471 ,  0.85939036,  0.85939036,\n",
       "         0.83677483,  0.83677483,  0.83579154,  0.83579154,  0.89282203,\n",
       "         0.89282203,  0.89675516,  0.89675516,  0.88446411,  0.88446411,\n",
       "         0.92379548,  0.92379548,  0.91642085,  0.91642085,  0.91986234,\n",
       "         0.91986234,  0.88593904,  0.88593904,  0.92920354,  0.92920354,\n",
       "         0.9346116 ,  0.9346116 ,  0.91887906,  0.91887906,  0.93706981,\n",
       "         0.93706981,  0.93313668,  0.93313668,  0.91740413,  0.91740413,\n",
       "         0.92330383,  0.92330383,  0.92477876,  0.92477876,  0.80678466,\n",
       "         0.80678466,  0.83677483,  0.83677483,  0.82890855,  0.82890855,\n",
       "         0.87168142,  0.87168142,  0.88151426,  0.88151426,  0.87413963,\n",
       "         0.87413963,  0.92084562,  0.92084562,  0.90953786,  0.90953786,\n",
       "         0.9100295 ,  0.9100295 ,  0.86479843,  0.86479843,  0.88741396,\n",
       "         0.88741396,  0.89036382,  0.89036382,  0.91297935,  0.91297935,\n",
       "         0.93117011,  0.93117011,  0.92379548,  0.92379548,  0.92182891,\n",
       "         0.92182891,  0.92625369,  0.92625369,  0.92772861,  0.92772861,\n",
       "         0.89233038,  0.89233038,  0.92576205,  0.92576205,  0.93067847,\n",
       "         0.93067847,  0.91494592,  0.91494592,  0.94690265,  0.94690265,\n",
       "         0.94247788,  0.94247788,  0.92625369,  0.92625369,  0.93559489,\n",
       "         0.93559489,  0.92576205,  0.92576205]),\n",
       " 'split3_train_score': array([ 0.81636591,  0.81636591,  0.79883101,  0.79883101,  0.79258881,\n",
       "         0.79258881,  0.86352287,  0.86352287,  0.88571104,  0.88571104,\n",
       "         0.89547157,  0.89547157,  0.94501192,  0.94501192,  0.94870049,\n",
       "         0.94870049,  0.95482919,  0.95482919,  0.93962093,  0.93962093,\n",
       "         0.9782658 ,  0.9782658 ,  0.98314607,  0.98314607,  0.9846215 ,\n",
       "         0.9846215 ,  0.99755987,  0.99755987,  0.99897855,  0.99897855,\n",
       "         0.99421178,  0.99421178,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99948927,  0.99948927,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99971626,  0.99971626,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99954602,  0.99954602,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.88735671,\n",
       "         0.88735671,  0.93218704,  0.93218704,  0.93950743,  0.93950743,\n",
       "         0.93337873,  0.93337873,  0.97139939,  0.97139939,  0.97605266,\n",
       "         0.97605266,  0.96107139,  0.96107139,  0.97049143,  0.97049143,\n",
       "         0.9721371 ,  0.9721371 ,  0.97156963,  0.97156963,  0.99642492,\n",
       "         0.99642492,  0.99841108,  0.99841108,  0.98791284,  0.98791284,\n",
       "         0.99909204,  0.99909204,  1.        ,  1.        ,  0.99540347,\n",
       "         0.99540347,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99994325,  0.99994325,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99920554,  0.99920554,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99897855,  0.99897855,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split4_test_score': array([ 0.78134557,  0.78134557,  0.75127421,  0.75127421,  0.74719674,\n",
       "         0.74719674,  0.8695209 ,  0.8695209 ,  0.87155963,  0.87155963,\n",
       "         0.87359837,  0.87359837,  0.89092762,  0.89092762,  0.91284404,\n",
       "         0.91284404,  0.91539246,  0.91539246,  0.86289501,  0.86289501,\n",
       "         0.8853211 ,  0.8853211 ,  0.88481142,  0.88481142,  0.90163099,\n",
       "         0.90163099,  0.91845056,  0.91845056,  0.91692151,  0.91692151,\n",
       "         0.93017329,  0.93017329,  0.93272171,  0.93272171,  0.93221203,\n",
       "         0.93221203,  0.87665647,  0.87665647,  0.93017329,  0.93017329,\n",
       "         0.93527013,  0.93527013,  0.90723751,  0.90723751,  0.93832824,\n",
       "         0.93832824,  0.94240571,  0.94240571,  0.92252803,  0.92252803,\n",
       "         0.93781855,  0.93781855,  0.93476045,  0.93476045,  0.86442406,\n",
       "         0.86442406,  0.87869521,  0.87869521,  0.87308869,  0.87308869,\n",
       "         0.89347604,  0.89347604,  0.91845056,  0.91845056,  0.91743119,\n",
       "         0.91743119,  0.91437309,  0.91437309,  0.93730887,  0.93730887,\n",
       "         0.92813456,  0.92813456,  0.86391437,  0.86391437,  0.90927625,\n",
       "         0.90927625,  0.9204893 ,  0.9204893 ,  0.91641182,  0.91641182,\n",
       "         0.94240571,  0.94240571,  0.94240571,  0.94240571,  0.9235474 ,\n",
       "         0.9235474 ,  0.94087666,  0.94087666,  0.94189602,  0.94189602,\n",
       "         0.88685015,  0.88685015,  0.94444444,  0.94444444,  0.93985729,\n",
       "         0.93985729,  0.90316004,  0.90316004,  0.95005097,  0.95005097,\n",
       "         0.94699286,  0.94699286,  0.9266055 ,  0.9266055 ,  0.94291539,\n",
       "         0.94291539,  0.94087666,  0.94087666]),\n",
       " 'split4_train_score': array([ 0.8155872 ,  0.8155872 ,  0.79218944,  0.79218944,  0.78908104,\n",
       "         0.78908104,  0.89352323,  0.89352323,  0.89097999,  0.89097999,\n",
       "         0.89504917,  0.89504917,  0.92856335,  0.92856335,  0.94178818,\n",
       "         0.94178818,  0.94540522,  0.94540522,  0.93879281,  0.93879281,\n",
       "         0.974059  ,  0.974059  ,  0.97609359,  0.97609359,  0.97643269,\n",
       "         0.97643269,  0.9979089 ,  0.9979089 ,  0.99824799,  0.99824799,\n",
       "         0.99253984,  0.99253984,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99954787,  0.99954787,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99920877,  0.99920877,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99892619,  0.99892619,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89312761,\n",
       "         0.89312761,  0.93059794,  0.93059794,  0.92952413,  0.92952413,\n",
       "         0.9408274 ,  0.9408274 ,  0.97225048,  0.97225048,  0.97479372,\n",
       "         0.97479372,  0.96111676,  0.96111676,  0.97032893,  0.97032893,\n",
       "         0.97049847,  0.97049847,  0.97061151,  0.97061151,  0.99214423,\n",
       "         0.99214423,  0.99768283,  0.99768283,  0.99270939,  0.99270939,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99626992,\n",
       "         0.99626992,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.9996609 ,  0.9996609 ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99903922,  0.99903922,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99903922,  0.99903922,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split5_test_score': array([ 0.79854621,  0.79854621,  0.7694704 ,  0.7694704 ,  0.7596054 ,\n",
       "         0.7596054 ,  0.8219107 ,  0.8219107 ,  0.85514019,  0.85514019,\n",
       "         0.85773624,  0.85773624,  0.90031153,  0.90031153,  0.90498442,\n",
       "         0.90498442,  0.90342679,  0.90342679,  0.84579439,  0.84579439,\n",
       "         0.86396677,  0.86396677,  0.86448598,  0.86448598,  0.89563863,\n",
       "         0.89563863,  0.91433022,  0.91433022,  0.92419522,  0.92419522,\n",
       "         0.90498442,  0.90498442,  0.91952233,  0.91952233,  0.92419522,\n",
       "         0.92419522,  0.89096573,  0.89096573,  0.93613707,  0.93613707,\n",
       "         0.9470405 ,  0.9470405 ,  0.90965732,  0.90965732,  0.93302181,\n",
       "         0.93302181,  0.94496366,  0.94496366,  0.90706127,  0.90706127,\n",
       "         0.92107996,  0.92107996,  0.92938733,  0.92938733,  0.81619938,\n",
       "         0.81619938,  0.86500519,  0.86500519,  0.86137072,  0.86137072,\n",
       "         0.90965732,  0.90965732,  0.91588785,  0.91588785,  0.90913811,\n",
       "         0.90913811,  0.90654206,  0.90654206,  0.91069574,  0.91069574,\n",
       "         0.91069574,  0.91069574,  0.88577362,  0.88577362,  0.9184839 ,\n",
       "         0.9184839 ,  0.93354102,  0.93354102,  0.89719626,  0.89719626,\n",
       "         0.94807892,  0.94807892,  0.9470405 ,  0.9470405 ,  0.91173416,\n",
       "         0.91173416,  0.91952233,  0.91952233,  0.92679128,  0.92679128,\n",
       "         0.89823468,  0.89823468,  0.94807892,  0.94807892,  0.95119418,\n",
       "         0.95119418,  0.89979232,  0.89979232,  0.95379024,  0.95379024,\n",
       "         0.94859813,  0.94859813,  0.91173416,  0.91173416,  0.92367601,\n",
       "         0.92367601,  0.92679128,  0.92679128]),\n",
       " 'split5_train_score': array([ 0.82701636,  0.82701636,  0.79464185,  0.79464185,  0.79075014,\n",
       "         0.79075014,  0.8679639 ,  0.8679639 ,  0.89684151,  0.89684151,\n",
       "         0.89413424,  0.89413424,  0.94043993,  0.94043993,  0.94732092,\n",
       "         0.94732092,  0.94495206,  0.94495206,  0.9606881 ,  0.9606881 ,\n",
       "         0.97292724,  0.97292724,  0.97997744,  0.97997744,  0.98076706,\n",
       "         0.98076706,  0.99249859,  0.99249859,  0.99576988,  0.99576988,\n",
       "         0.99791314,  0.99791314,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99943598,  0.99943598,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99954879,  0.99954879,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99943598,  0.99943598,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.88398195,\n",
       "         0.88398195,  0.93767625,  0.93767625,  0.93333333,  0.93333333,\n",
       "         0.94359842,  0.94359842,  0.96474901,  0.96474901,  0.96672307,\n",
       "         0.96672307,  0.96209814,  0.96209814,  0.97005076,  0.97005076,\n",
       "         0.97292724,  0.97292724,  0.9714608 ,  0.9714608 ,  0.99509306,\n",
       "         0.99509306,  0.99881557,  0.99881557,  0.9892273 ,  0.9892273 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99825155,\n",
       "         0.99830795,  0.99949239,  0.99949239,  1.        ,  1.        ,\n",
       "         0.99881557,  0.99881557,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99966159,  0.99966159,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99915398,  0.99915398,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split6_test_score': array([ 0.80529101,  0.80529101,  0.77142857,  0.77142857,  0.76666667,\n",
       "         0.76666667,  0.84708995,  0.84708995,  0.86031746,  0.86031746,\n",
       "         0.86560847,  0.86560847,  0.9015873 ,  0.9015873 ,  0.9021164 ,\n",
       "         0.9021164 ,  0.9047619 ,  0.9047619 ,  0.86349206,  0.86349206,\n",
       "         0.87989418,  0.87989418,  0.87513228,  0.87513228,  0.89470899,\n",
       "         0.89470899,  0.91111111,  0.91111111,  0.91111111,  0.91111111,\n",
       "         0.9042328 ,  0.9042328 ,  0.92380952,  0.92380952,  0.92645503,\n",
       "         0.92645503,  0.88783069,  0.88783069,  0.94338624,  0.94338624,\n",
       "         0.93862434,  0.93862434,  0.91587302,  0.91587302,  0.94920635,\n",
       "         0.94920635,  0.93756614,  0.93756614,  0.92010582,  0.92010582,\n",
       "         0.92962963,  0.92962963,  0.92698413,  0.92698413,  0.85449735,\n",
       "         0.85449735,  0.85396825,  0.85396825,  0.85661376,  0.85661376,\n",
       "         0.87883598,  0.87883598,  0.9037037 ,  0.9037037 ,  0.90740741,\n",
       "         0.90740741,  0.90952381,  0.90952381,  0.91534392,  0.91534392,\n",
       "         0.91746032,  0.91746032,  0.87089947,  0.87089947,  0.8962963 ,\n",
       "         0.8962963 ,  0.92063492,  0.92063492,  0.90634921,  0.90634921,\n",
       "         0.93544974,  0.93544974,  0.93703704,  0.93703704,  0.93280423,\n",
       "         0.93280423,  0.92857143,  0.92857143,  0.93068783,  0.93068783,\n",
       "         0.8968254 ,  0.8968254 ,  0.93915344,  0.93915344,  0.94232804,\n",
       "         0.94232804,  0.92328042,  0.92328042,  0.93862434,  0.93862434,\n",
       "         0.94814815,  0.94814815,  0.92169312,  0.92169312,  0.93968254,\n",
       "         0.93968254,  0.93703704,  0.93703704]),\n",
       " 'split6_train_score': array([ 0.82854891,  0.82854891,  0.81143758,  0.81143758,  0.78897895,\n",
       "         0.78897895,  0.87008893,  0.87008893,  0.89063379,  0.89063379,\n",
       "         0.89463019,  0.89463019,  0.93442531,  0.93442531,  0.93825284,\n",
       "         0.93825284,  0.94326241,  0.94326241,  0.93949116,  0.93949116,\n",
       "         0.9738827 ,  0.9738827 ,  0.98108747,  0.98108747,  0.97264438,\n",
       "         0.97264438,  0.99639761,  0.99639761,  0.99870539,  0.99870539,\n",
       "         0.9955533 ,  0.9955533 ,  0.99949341,  0.99949341,  0.99949341,\n",
       "         0.99949341,  0.99938084,  0.99938084,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99943713,  0.99943713,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99859282,  0.99859282,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89074637,\n",
       "         0.89074637,  0.92885287,  0.92885287,  0.93504447,  0.93504447,\n",
       "         0.94095463,  0.94095463,  0.96555218,  0.96555218,  0.97101205,\n",
       "         0.97101205,  0.96617134,  0.96617134,  0.96831026,  0.96831026,\n",
       "         0.97219408,  0.97219408,  0.96521445,  0.96521445,  0.99673534,\n",
       "         0.99673534,  0.99808623,  0.99808623,  0.98992458,  0.98992458,\n",
       "         0.99949341,  0.99949341,  1.        ,  1.        ,  0.99791737,\n",
       "         0.99791737,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99938084,  0.99938084,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99988743,  0.99988743,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9990994 ,  0.9990994 ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split7_test_score': array([ 0.8034188 ,  0.8034188 ,  0.78846154,  0.78846154,  0.78365385,\n",
       "         0.78365385,  0.84401709,  0.84401709,  0.8707265 ,  0.8707265 ,\n",
       "         0.86538462,  0.86538462,  0.89636752,  0.89636752,  0.90010684,\n",
       "         0.90010684,  0.90438034,  0.90438034,  0.87767094,  0.87767094,\n",
       "         0.85897436,  0.85897436,  0.875     ,  0.875     ,  0.90491453,\n",
       "         0.90491453,  0.91559829,  0.91559829,  0.91346154,  0.91346154,\n",
       "         0.91666667,  0.91666667,  0.92788462,  0.92788462,  0.92574786,\n",
       "         0.92574786,  0.88621795,  0.88621795,  0.94070513,  0.94070513,\n",
       "         0.94444444,  0.94444444,  0.91346154,  0.91346154,  0.92681624,\n",
       "         0.92681624,  0.94871795,  0.94871795,  0.92040598,  0.92040598,\n",
       "         0.92895299,  0.92895299,  0.92948718,  0.92948718,  0.85737179,\n",
       "         0.85737179,  0.86431624,  0.86431624,  0.87339744,  0.87339744,\n",
       "         0.89690171,  0.89690171,  0.9107906 ,  0.9107906 ,  0.89957265,\n",
       "         0.89957265,  0.91880342,  0.91880342,  0.92361111,  0.92361111,\n",
       "         0.92628205,  0.92628205,  0.88675214,  0.88675214,  0.91613248,\n",
       "         0.91613248,  0.92094017,  0.92094017,  0.89797009,  0.89797009,\n",
       "         0.92361111,  0.92361111,  0.93963675,  0.93963675,  0.93055556,\n",
       "         0.93055556,  0.92681624,  0.92681624,  0.9332265 ,  0.9332265 ,\n",
       "         0.89423077,  0.89423077,  0.93536325,  0.93536325,  0.94123932,\n",
       "         0.94123932,  0.89903846,  0.89903846,  0.94284188,  0.94284188,\n",
       "         0.9465812 ,  0.9465812 ,  0.92521368,  0.92521368,  0.93376068,\n",
       "         0.93376068,  0.93589744,  0.93589744]),\n",
       " 'split7_train_score': array([ 0.83760684,  0.83760684,  0.80718623,  0.80718623,  0.79520918,\n",
       "         0.79520918,  0.87477508,  0.87477508,  0.88860774,  0.88860774,\n",
       "         0.89822312,  0.89822312,  0.94168916,  0.94168916,  0.94140801,\n",
       "         0.94140801,  0.94000225,  0.94000225,  0.94146424,  0.94146424,\n",
       "         0.97435897,  0.97435897,  0.97970085,  0.97970085,  0.97615834,\n",
       "         0.97615834,  0.99741341,  0.99741341,  0.99836932,  0.99836932,\n",
       "         0.99713225,  0.99713225,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99865047,  0.99865047,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99921278,  0.99921278,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99938147,  0.99938147,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89721098,\n",
       "         0.89721098,  0.92645074,  0.92645074,  0.93280477,  0.93280477,\n",
       "         0.93904633,  0.93904633,  0.97542735,  0.97542735,  0.96727395,\n",
       "         0.96727395,  0.96232569,  0.96232569,  0.97188484,  0.97188484,\n",
       "         0.97239091,  0.97239091,  0.97194107,  0.97194107,  0.9957265 ,\n",
       "         0.9957265 ,  0.99820063,  0.99820063,  0.99139676,  0.99139676,\n",
       "         0.99932524,  0.99932524,  1.        ,  1.        ,  0.99578273,\n",
       "         0.99578273,  0.99955016,  0.99955016,  1.        ,  1.        ,\n",
       "         0.99977508,  0.99977508,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99893162,  0.99893162,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99910031,  0.99910031,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split8_test_score': array([ 0.798274  ,  0.798274  ,  0.78910464,  0.78910464,  0.77346278,\n",
       "         0.77346278,  0.85544768,  0.85544768,  0.86839266,  0.86839266,\n",
       "         0.86731392,  0.86731392,  0.88727077,  0.88727077,  0.89158576,\n",
       "         0.89158576,  0.88511327,  0.88511327,  0.87001079,  0.87001079,\n",
       "         0.8635383 ,  0.8635383 ,  0.86245955,  0.86245955,  0.88403452,\n",
       "         0.88403452,  0.89859763,  0.89859763,  0.89536138,  0.89536138,\n",
       "         0.92394822,  0.92394822,  0.91531823,  0.91531823,  0.91531823,\n",
       "         0.91531823,  0.87864078,  0.87864078,  0.91423948,  0.91423948,\n",
       "         0.92826321,  0.92826321,  0.90399137,  0.90399137,  0.92610572,\n",
       "         0.92610572,  0.92934196,  0.92934196,  0.92718447,  0.92718447,\n",
       "         0.92340885,  0.92340885,  0.92071197,  0.92071197,  0.84735707,\n",
       "         0.84735707,  0.85868393,  0.85868393,  0.8592233 ,  0.8592233 ,\n",
       "         0.899137  ,  0.899137  ,  0.89644013,  0.89644013,  0.89050701,\n",
       "         0.89050701,  0.91909385,  0.91909385,  0.91262136,  0.91262136,\n",
       "         0.91693635,  0.91693635,  0.86785329,  0.86785329,  0.89859763,\n",
       "         0.89859763,  0.89805825,  0.89805825,  0.91963323,  0.91963323,\n",
       "         0.92340885,  0.92340885,  0.92718447,  0.92718447,  0.93096009,\n",
       "         0.93096009,  0.92394822,  0.92394822,  0.92664509,  0.92664509,\n",
       "         0.88888889,  0.88888889,  0.93527508,  0.93527508,  0.93149946,\n",
       "         0.93149946,  0.91639698,  0.91639698,  0.93096009,  0.93096009,\n",
       "         0.92610572,  0.92610572,  0.92556634,  0.92556634,  0.92934196,\n",
       "         0.92934196,  0.92286947,  0.92286947]),\n",
       " 'split8_train_score': array([ 0.80827997,  0.80827997,  0.79401191,  0.79401191,  0.78766431,\n",
       "         0.78766431,  0.88686664,  0.88686664,  0.90231435,  0.90231435,\n",
       "         0.89321425,  0.89321425,  0.93023256,  0.93023256,  0.93697337,\n",
       "         0.93697337,  0.93736659,  0.93736659,  0.9468599 ,  0.9468599 ,\n",
       "         0.97753061,  0.97753061,  0.98410291,  0.98410291,  0.98865296,\n",
       "         0.98865296,  0.99601168,  0.99601168,  0.99702281,  0.99702281,\n",
       "         0.99589934,  0.99589934,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99994383,  0.99994383,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99882036,  0.99882036,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.99904505,  0.99904505,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.8892821 ,\n",
       "         0.8892821 ,  0.9269745 ,  0.9269745 ,  0.93978205,  0.93978205,\n",
       "         0.9489945 ,  0.9489945 ,  0.96517245,  0.96517245,  0.97084597,\n",
       "         0.97084597,  0.96798113,  0.96798113,  0.97320526,  0.97320526,\n",
       "         0.97196944,  0.97196944,  0.96904842,  0.96904842,  0.99747219,\n",
       "         0.99747219,  0.99921357,  0.99921357,  0.99427031,  0.99427031,\n",
       "         0.99983148,  0.99983148,  1.        ,  1.        ,  0.99668577,\n",
       "         0.99668577,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.99938209,  0.99938209,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99971913,  0.99971913,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99898888,  0.99898888,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'split9_test_score': array([ 0.80363036,  0.80363036,  0.78272827,  0.78272827,  0.7640264 ,\n",
       "         0.7640264 ,  0.87953795,  0.87953795,  0.87238724,  0.87238724,\n",
       "         0.87788779,  0.87788779,  0.89273927,  0.89273927,  0.91309131,\n",
       "         0.91309131,  0.9070407 ,  0.9070407 ,  0.84268427,  0.84268427,\n",
       "         0.8630363 ,  0.8630363 ,  0.85753575,  0.85753575,  0.89328933,\n",
       "         0.89328933,  0.91419142,  0.91419142,  0.9119912 ,  0.9119912 ,\n",
       "         0.93179318,  0.93179318,  0.93949395,  0.93949395,  0.94169417,\n",
       "         0.94169417,  0.89383938,  0.89383938,  0.94554455,  0.94554455,\n",
       "         0.94609461,  0.94609461,  0.93894389,  0.93894389,  0.95489549,\n",
       "         0.95489549,  0.95489549,  0.95489549,  0.93784378,  0.93784378,\n",
       "         0.9449945 ,  0.9449945 ,  0.9449945 ,  0.9449945 ,  0.82838284,\n",
       "         0.82838284,  0.8679868 ,  0.8679868 ,  0.85588559,  0.85588559,\n",
       "         0.88613861,  0.88613861,  0.89988999,  0.89988999,  0.9009901 ,\n",
       "         0.9009901 ,  0.93454345,  0.93454345,  0.93454345,  0.93454345,\n",
       "         0.9290429 ,  0.9290429 ,  0.88888889,  0.88888889,  0.89548955,\n",
       "         0.89548955,  0.90869087,  0.90869087,  0.92849285,  0.92849285,\n",
       "         0.94059406,  0.94059406,  0.94114411,  0.94114411,  0.94444444,\n",
       "         0.94444444,  0.94664466,  0.94664466,  0.9449945 ,  0.9449945 ,\n",
       "         0.89713971,  0.89713971,  0.94444444,  0.94444444,  0.9510451 ,\n",
       "         0.9510451 ,  0.91969197,  0.91969197,  0.95709571,  0.95709571,\n",
       "         0.9510451 ,  0.9510451 ,  0.93729373,  0.93729373,  0.94444444,\n",
       "         0.94444444,  0.94609461,  0.94609461]),\n",
       " 'split9_train_score': array([ 0.82296222,  0.82296222,  0.81225474,  0.81225474,  0.79089584,\n",
       "         0.79089584,  0.90183877,  0.90183877,  0.89634488,  0.89634488,\n",
       "         0.89791456,  0.89791456,  0.94635049,  0.94635049,  0.94948985,\n",
       "         0.94948985,  0.94158538,  0.94158538,  0.9422581 ,  0.9422581 ,\n",
       "         0.97690324,  0.97690324,  0.98116381,  0.98116381,  0.97600628,\n",
       "         0.97600628,  0.99405763,  0.99405763,  0.99551519,  0.99551519,\n",
       "         0.99557125,  0.99557125,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99904698,  0.99904698,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.99966364,  0.99966364,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  0.9994394 ,  0.9994394 ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.89584034,\n",
       "         0.89584034,  0.92672945,  0.92672945,  0.93205516,  0.93205516,\n",
       "         0.9422581 ,  0.9422581 ,  0.96227156,  0.96227156,  0.96490638,\n",
       "         0.96490638,  0.96176701,  0.96176701,  0.97230631,  0.97230631,\n",
       "         0.97325933,  0.97325933,  0.9752775 ,  0.9752775 ,  0.99702881,\n",
       "         0.99702881,  0.99826214,  0.99826214,  0.99282431,  0.99282431,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  0.99742124,\n",
       "         0.99742124,  1.        ,  1.        ,  0.99955152,  0.99955152,\n",
       "         0.9991591 ,  0.9991591 ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  0.99949546,  0.99949546,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  0.9991591 ,  0.9991591 ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ]),\n",
       " 'std_fit_time': array([ 0.02066456,  0.02340019,  0.0532817 ,  0.04735302,  0.09431387,\n",
       "         0.16471136,  0.01520312,  0.0293498 ,  0.02903013,  0.01049569,\n",
       "         0.24623119,  0.39269807,  0.10885791,  0.08589857,  0.37799628,\n",
       "         0.42440382,  1.52781425,  3.26996799,  0.04649776,  0.02203692,\n",
       "         0.08308988,  0.05439816,  0.29566293,  0.48874455,  0.03069174,\n",
       "         0.01642104,  0.12011393,  0.12038539,  0.69435141,  0.90996487,\n",
       "         0.19119828,  0.20626496,  0.49698677,  0.69907979,  4.38654073,\n",
       "         5.03208397,  0.07115835,  0.0362319 ,  0.13021203,  0.13980754,\n",
       "         0.46630003,  0.4705552 ,  0.0628446 ,  0.01454669,  0.23359579,\n",
       "         0.05869127,  0.61642258,  1.08336677,  0.30003092,  0.28270577,\n",
       "         0.34796833,  0.47574788,  6.91774686,  6.77040813,  0.04266527,\n",
       "         0.02389079,  0.06034311,  0.04905881,  0.14366905,  0.09509646,\n",
       "         0.02017661,  0.02116072,  0.06978633,  0.02022535,  0.24072441,\n",
       "         0.38405317,  0.13030869,  0.04765299,  0.24298016,  0.16424879,\n",
       "         1.13602476,  0.92805733,  0.04784408,  0.01469465,  0.05577214,\n",
       "         0.03964435,  0.17537955,  0.19806254,  0.02250578,  0.02510493,\n",
       "         0.0728616 ,  0.06642082,  0.60695592,  0.48793125,  0.18232601,\n",
       "         0.16582305,  0.3038642 ,  0.32272204,  4.62998943,  5.06411625,\n",
       "         0.0451168 ,  0.0533221 ,  0.10427194,  0.06711779,  0.55208233,\n",
       "         0.4772068 ,  0.04076551,  0.04935584,  0.17557661,  0.10338136,\n",
       "         0.53692006,  0.94567545,  0.22466664,  0.1832259 ,  0.50950096,\n",
       "         0.83016666,  1.61961511,  1.04763786]),\n",
       " 'std_score_time': array([ 0.0015168 ,  0.00179411,  0.00119639,  0.00128383,  0.00046821,\n",
       "         0.04890295,  0.0009885 ,  0.0014787 ,  0.00978341,  0.01204485,\n",
       "         0.05772713,  0.05804509,  0.01617586,  0.01716462,  0.01665668,\n",
       "         0.02192871,  0.10301213,  0.08087682,  0.01108318,  0.0166689 ,\n",
       "         0.00253238,  0.00727984,  0.05633609,  0.06210716,  0.00702367,\n",
       "         0.01266649,  0.01354001,  0.0131388 ,  0.09295934,  0.10713829,\n",
       "         0.03031943,  0.03070762,  0.01882235,  0.01917069,  0.05698   ,\n",
       "         0.09609632,  0.01539626,  0.01065082,  0.00791285,  0.01037011,\n",
       "         0.07954546,  0.05990145,  0.01377416,  0.00699611,  0.01179523,\n",
       "         0.01617511,  0.05274681,  0.04279493,  0.01187353,  0.01551695,\n",
       "         0.03546249,  0.04446715,  0.09202672,  0.1356639 ,  0.01606232,\n",
       "         0.00247132,  0.00880255,  0.00655164,  0.03778053,  0.04558724,\n",
       "         0.00171095,  0.00199682,  0.01035917,  0.00617414,  0.0509592 ,\n",
       "         0.06162326,  0.01672422,  0.01537112,  0.01501739,  0.01633052,\n",
       "         0.09530127,  0.10380106,  0.02016113,  0.00646959,  0.00600787,\n",
       "         0.00275031,  0.03114659,  0.04487524,  0.00672729,  0.00343242,\n",
       "         0.01065461,  0.01054024,  0.04533012,  0.01592945,  0.01214638,\n",
       "         0.01500756,  0.02559787,  0.02113724,  0.0357275 ,  0.10786885,\n",
       "         0.02214222,  0.01553368,  0.0152528 ,  0.01053769,  0.07617886,\n",
       "         0.05109099,  0.02151982,  0.0108391 ,  0.01798755,  0.01410895,\n",
       "         0.03614287,  0.08478591,  0.01340213,  0.01496176,  0.01787664,\n",
       "         0.03581732,  0.05064804,  0.07012895]),\n",
       " 'std_test_score': array([ 0.01736258,  0.01736258,  0.01587302,  0.01587302,  0.01460712,\n",
       "         0.01460712,  0.02490311,  0.02490311,  0.01657152,  0.01657152,\n",
       "         0.0141353 ,  0.0141353 ,  0.01009416,  0.01009416,  0.01020541,\n",
       "         0.01020541,  0.01025343,  0.01025343,  0.01709795,  0.01709795,\n",
       "         0.01565707,  0.01565707,  0.01534212,  0.01534212,  0.00755119,\n",
       "         0.00755119,  0.00954457,  0.00954457,  0.01184762,  0.01184762,\n",
       "         0.01209852,  0.01209852,  0.00906248,  0.00906248,  0.00807793,\n",
       "         0.00807793,  0.00620466,  0.00620466,  0.00915119,  0.00915119,\n",
       "         0.00577356,  0.00577356,  0.00906156,  0.00906156,  0.00886359,\n",
       "         0.00886359,  0.00775841,  0.00775841,  0.00904618,  0.00904618,\n",
       "         0.00995134,  0.00995134,  0.00744814,  0.00744814,  0.01742131,\n",
       "         0.01742131,  0.01313141,  0.01313141,  0.01547179,  0.01547179,\n",
       "         0.01428561,  0.01428561,  0.01370107,  0.01370107,  0.01271476,\n",
       "         0.01271476,  0.00927478,  0.00927478,  0.01220895,  0.01220895,\n",
       "         0.00935269,  0.00935269,  0.01171495,  0.01171495,  0.01219203,\n",
       "         0.01219203,  0.01214033,  0.01214033,  0.01013494,  0.01013494,\n",
       "         0.00808112,  0.00808112,  0.0070046 ,  0.0070046 ,  0.00837793,\n",
       "         0.00837793,  0.00877903,  0.00877903,  0.00745601,  0.00745601,\n",
       "         0.0098744 ,  0.0098744 ,  0.01125836,  0.01125836,  0.00695395,\n",
       "         0.00695395,  0.00799364,  0.00799364,  0.00870143,  0.00870143,\n",
       "         0.0072984 ,  0.0072984 ,  0.00719262,  0.00719262,  0.00779755,\n",
       "         0.00779755,  0.00840441,  0.00840441]),\n",
       " 'std_train_score': array([ 0.00931277,  0.00931277,  0.00711265,  0.00711265,  0.002535  ,\n",
       "         0.002535  ,  0.0147788 ,  0.0147788 ,  0.00486581,  0.00486581,\n",
       "         0.00283201,  0.00283201,  0.00575409,  0.00575409,  0.00460433,\n",
       "         0.00460433,  0.00541564,  0.00541564,  0.00700627,  0.00700627,\n",
       "         0.00210099,  0.00210099,  0.00209192,  0.00209192,  0.00645964,\n",
       "         0.00645964,  0.00184166,  0.00184166,  0.00120765,  0.00120765,\n",
       "         0.00160799,  0.00160799,  0.00019291,  0.00019291,  0.00015198,\n",
       "         0.00015198,  0.0003642 ,  0.0003642 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.0002647 ,  0.0002647 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00031075,  0.00031075,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00521846,\n",
       "         0.00521846,  0.00426285,  0.00426285,  0.00316895,  0.00316895,\n",
       "         0.00703701,  0.00703701,  0.0045781 ,  0.0045781 ,  0.00333724,\n",
       "         0.00333724,  0.00557432,  0.00557432,  0.00309272,  0.00309272,\n",
       "         0.00184974,  0.00184974,  0.00317222,  0.00317222,  0.00156977,\n",
       "         0.00156977,  0.00049129,  0.00049129,  0.00255164,  0.00255164,\n",
       "         0.00037808,  0.00037808,  0.        ,  0.        ,  0.00201241,\n",
       "         0.00201811,  0.00019193,  0.00019193,  0.00013454,  0.00013454,\n",
       "         0.00031575,  0.00031575,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00036786,  0.00036786,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00022051,  0.00022051,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
